[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From Zero to Production: A Practical Python Development Pipeline",
    "section": "",
    "text": "1 Introduction\nThe Python ecosystem has grown tremendously over the past decade, bringing with it an explosion of tools, frameworks, and practices. While this rich ecosystem offers powerful capabilities, it often leaves developers—especially those new to Python—feeling overwhelmed by choice paralysis. Which virtual environment tool should I use? How should I format my code? What’s the best way to manage dependencies? How do I set up testing? The questions seem endless.\nThis guide aims to cut through the noise by presenting a comprehensive, end-to-end development pipeline that strikes a deliberate balance between simplicity and effectiveness. Rather than showcasing every possible tool, we focus on the vital 80/20 solution: the 20% of practices that yield 80% of the benefits.\nWhether you’re a beginner taking your first steps beyond basic scripts, an intermediate developer looking to professionalize your workflow, or an educator teaching best practices, this guide provides a clear path forward. We’ll build this pipeline in stages:\nThroughout this journey, we’ll introduce tools and practices that scale with your needs. We’ll start with simpler approaches and progress to more robust solutions, letting you decide when to adopt more advanced techniques based on your project’s complexity. A theme throughout the book is ‘Simple but no Simplistic’.\nTo help you quickly apply these practices, we’ve created a companion cookiecutter template that automatically sets up a new Python project with the recommended structure and configurations. You can find this template at [GitHub repository URL] and use it to jumpstart your projects with best practices already in place. We’ll discuss how to use and customize this template throughout the guide.\nImportantly, this isn’t just about tools—it’s about building habits and workflows that make development more enjoyable and productive. The practices we’ll explore enhance code quality and team collaboration without unnecessary complexity, creating a foundation you can build upon as your skills and projects grow."
  },
  {
    "objectID": "index.html#the-evolving-python-ecosystem-ai-as-a-development-partner",
    "href": "index.html#the-evolving-python-ecosystem-ai-as-a-development-partner",
    "title": "From Zero to Production: A Practical Python Development Pipeline",
    "section": "1.1 The Evolving Python Ecosystem: AI as a Development Partner",
    "text": "1.1 The Evolving Python Ecosystem: AI as a Development Partner\nThe Python development landscape has expanded to include AI-powered tools that enhance developer productivity. These tools - ranging from code completion systems to large language models (LLMs) that can answer complex questions - don’t replace traditional development practices but rather augment them.\nAs you progress through this guide, you’ll notice references to how AI assistants can support various aspects of the development process. Whether generating boilerplate code, suggesting test cases, or helping troubleshoot complex errors, these tools represent a significant shift in how developers work. While AI assistance brings substantial benefits, it works best when paired with strong fundamentals and critical evaluation - exactly the skills this guide aims to build.\nThe practices we cover remain essential regardless of whether you use AI tools. Understanding project structure, testing principles, and code quality isn’t obsolete - if anything, these fundamentals become more important as you leverage AI to accelerate your workflow.\nYes, including a paragraph about editors in the main document would be valuable. I suggest adding a section near the beginning of the book (perhaps in the Introduction or early in Part 1) that acknowledges the role of editors in the development process while emphasizing your focus on editor-agnostic practices."
  },
  {
    "objectID": "index.html#development-environments-and-editor-choice",
    "href": "index.html#development-environments-and-editor-choice",
    "title": "From Zero to Production: A Practical Python Development Pipeline",
    "section": "1.2 Development Environments and Editor Choice",
    "text": "1.2 Development Environments and Editor Choice\nThroughout this guide, we focus on practices and workflows that remain consistent regardless of your chosen development environment. Whether you prefer a full-featured IDE like PyCharm, a lightweight but extensible editor like VS Code, or keyboard-centric tools like Vim or Emacs, the principles we cover apply universally.\nWhile your choice of editor can significantly impact your productivity, the fundamental aspects of Python development—project structure, version control, dependency management, testing, and deployment—remain consistent across environments. Most modern editors provide integration with the tools we’ll discuss, such as virtual environments, linters, formatters, and testing frameworks. Rather than prescribing specific editor configurations, this guide emphasizes the underlying practices that make for effective Python development.\nFor readers interested in editor-specific setups, Appendix J provides an overview of popular Python development environments and how they integrate with the tools covered in this book. This appendix includes configuration examples for common editors and tips for maximizing productivity in each environment."
  },
  {
    "objectID": "index.html#how-to-use-this-guide",
    "href": "index.html#how-to-use-this-guide",
    "title": "From Zero to Production: A Practical Python Development Pipeline",
    "section": "1.3 How to Use This Guide",
    "text": "1.3 How to Use This Guide\nThis guide is designed to accommodate different learning styles and experience levels. Depending on your preferences and needs, you might approach this document in different ways:\n\nSequential learners can work through Parts 1-3 in order, building their development pipeline step by step\nPractical learners might want to jump straight to Part 4 (the SimpleBot case study) and refer back to earlier sections as needed\nReference-oriented learners can use the appendices and workflow checklist as their primary resources\nVisual thinkers will find the workflow checklist particularly helpful for understanding the big picture\n\nWhile this guide focuses on Python, it’s worth noting that many of the core principles and practices discussed—version control, testing, documentation, CI/CD, code quality—apply across software development in general. We’ve chosen to demonstrate these concepts through Python due to its popularity and approachable syntax, but the workflow philosophy transcends any specific language. Developers working in other languages will find much of this guidance transferable to their environments, with adjustments for language-specific tools.\nThe guide is structured into four main parts, followed by appendices for quick reference:\n\nPart 1: Setting the Foundation - Covers project structure, version control, and virtual environments\nPart 2: Advancing Your Workflow - Explores dependency management, code quality tools, testing, and type checking\nPart 3: Documentation and Deployment - Discusses documentation options and CI/CD automation\nPart 4: Case Study - Building SimpleBot - Demonstrates applying these practices to a real project\nAppendices - Provide a workflow checklist, tools reference, and glossary of terms\n\nWhether you’re starting your first serious Python project or looking to professionalize an existing workflow, you’ll find relevant guidance throughout. Feel free to focus on the sections most applicable to your current needs and revisit others as your projects evolve.\nLet’s begin by setting up a solid foundation for your Python projects."
  },
  {
    "objectID": "chapters/01-foundation.html#python-project-structure-best-practices",
    "href": "chapters/01-foundation.html#python-project-structure-best-practices",
    "title": "2  Setting the Foundation",
    "section": "2.1 Python Project Structure Best Practices",
    "text": "2.1 Python Project Structure Best Practices\nA well-organized project structure is the cornerstone of maintainable Python code. Even before writing a single line of code, decisions about how to organize your files will impact how easily you can test, document, and expand your project.\nThe structure we recommend follows modern Python conventions, prioritizing clarity and separation of concerns:\nmy_project/\n├── src/                    # Main source code directory\n│   └── my_package/         # Your actual Python package\n│       ├── __init__.py     # Makes the directory a package\n│       ├── main.py         # Core functionality\n│       └── helpers.py      # Supporting functions/classes\n├── tests/                  # Test suite\n│   ├── __init__.py\n│   ├── test_main.py        # Tests for main.py\n│   └── test_helpers.py     # Tests for helpers.py\n├── docs/                   # Documentation (can start simple)\n│   └── index.md            # Main documentation page\n├── .gitignore              # Files to exclude from Git\n├── README.md               # Project overview and quick start\n├── requirements.in         # Direct dependencies (human-maintained)\n├── requirements.txt        # Locked dependencies (generated)\n└── pyproject.toml          # Tool configuration\n\n2.1.1 Why Use the src Layout?\nThe src layout (placing your package inside a src directory rather than at the project root) provides several advantages:\n\nEnforces proper installation: When developing, you must install your package to use it, ensuring you’re testing the same way users will experience it.\nPrevents accidental imports: You can’t accidentally import from your project without installing it, avoiding confusing behaviors.\nClarifies package boundaries: Makes it explicit which code is part of your distributable package.\n\nWhile simpler projects might skip this layout, adopting it early builds good habits and makes future growth easier.\n\n\n2.1.2 Key Components Explained\n\nsrc/my_package/: Contains your actual Python code. The package name should be unique and descriptive.\ntests/: Keeps tests separate from implementation but adjacent in the repository.\ndocs/: Houses documentation, starting simple and growing as needed.\n.gitignore: Tells Git which files to ignore (like virtual environments, cache files, etc.).\nREADME.md: The first document anyone will see—provide clear instructions on installation and basic usage.\nrequirements.in/requirements.txt: Manages dependencies (we’ll explain this approach in Part 2).\npyproject.toml: Configuration for development tools like Ruff and mypy, following modern standards.\n\n\n\n2.1.3 Getting Started\nCreating this structure is straightforward. Here’s how to initialize a basic project:\n# Create the project directory\nmkdir my_project && cd my_project\n\n# Create the basic structure\nmkdir -p src/my_package tests docs\n\n# Initialize the Python package\ntouch src/my_package/__init__.py\ntouch src/my_package/main.py\n\n# Create initial test files\ntouch tests/__init__.py\ntouch tests/test_main.py\n\n# Create essential files\necho \"# My Project\\nA short description of my project.\" &gt; README.md\ntouch requirements.in\ntouch pyproject.toml\n\n# Initialize Git repository\ngit init\nThis structure promotes maintainability and follows Python’s conventions. It might seem like overkill for tiny scripts, but as your project grows, you’ll appreciate having this organization from the start.\nIn the next section, we’ll build on this foundation by implementing version control best practices."
  },
  {
    "objectID": "chapters/01-foundation.html#version-control-fundamentals",
    "href": "chapters/01-foundation.html#version-control-fundamentals",
    "title": "2  Setting the Foundation",
    "section": "2.2 Version Control Fundamentals",
    "text": "2.2 Version Control Fundamentals\nVersion control is an essential part of modern software development, and Git has become the de facto standard. Even for small solo projects, proper version control offers invaluable benefits for tracking changes, experimenting safely, and maintaining a clear history of your work.\n\n2.2.1 Setting Up Git\nIf you haven’t set up Git yet, here’s how to get started:\n# Configure your identity (use your actual name and email)\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n# Initialize Git in your project (if not done already)\ngit init\n\n# Create a .gitignore file to exclude unnecessary files\nA good .gitignore file is essential for Python projects. Here’s a simplified version to start with:\n# Virtual environments\n.venv/\nvenv/\nenv/\n\n# Python cache files\n__pycache__/\n*.py[cod]\n*$py.class\n.pytest_cache/\n\n# Distribution / packaging\ndist/\nbuild/\n*.egg-info/\n\n# Local development settings\n.env\n.vscode/\n.idea/\n\n# Coverage reports\nhtmlcov/\n.coverage\n\n# Generated documentation\nsite/\n\n\n2.2.2 Basic Git Workflow\nFor beginners, a simple Git workflow is sufficient:\n\nMake changes to your code\nStage changes you want to commit\nCommit with a meaningful message\nPush to a remote repository (like GitHub)\n\nHere’s what this looks like in practice:\n# Check what files you've changed\ngit status\n\n# Stage specific files (or use git add . for all changes)\ngit add src/my_package/main.py tests/test_main.py\n\n# Commit changes with a descriptive message\ngit commit -m \"Add user authentication function and tests\"\n\n# Push to a remote repository (if using GitHub or similar)\ngit push origin main\n\n\n2.2.3 Effective Commit Messages\nGood commit messages are vital for understanding project history. Follow these simple guidelines:\n\nUse the imperative mood (“Add feature” not “Added feature”)\nKeep the first line under 50 characters as a summary\nWhen needed, add more details after a blank line\nExplain why a change was made, not just what changed\n\nExample of a good commit message:\nAdd password validation function\n\n- Implements minimum length of 8 characters\n- Requires at least one special character\n- Fixes #42 (weak password vulnerability)\n\n\n2.2.4 Branching for Features and Fixes\nAs your project grows, a branching workflow helps manage different streams of work:\n# Create a new branch for a feature\ngit checkout -b feature/user-profiles\n\n# Make changes, commit, and push to the branch\ngit add .\ngit commit -m \"Add user profile page\"\ngit push origin feature/user-profiles\n\n# When ready, merge back to main (after review)\ngit checkout main\ngit merge feature/user-profiles\nFor team projects, consider using pull/merge requests on platforms like GitHub or GitLab rather than direct merges to the main branch. This enables code review and discussion before changes are incorporated.\n\n\n2.2.5 Integrating with GitHub or GitLab\nHosting your repository on GitHub, GitLab, or similar services provides:\n\nA backup of your code\nCollaboration tools (issues, pull requests)\nIntegration with CI/CD services\nVisibility for your project\n\nTo connect your local repository to GitHub:\n# After creating a repository on GitHub\ngit remote add origin https://github.com/yourusername/my_project.git\ngit branch -M main\ngit push -u origin main\n\n\n2.2.6 Git Best Practices for Beginners\n\nCommit frequently: Small, focused commits are easier to understand and review\nNever commit sensitive data: Passwords, API keys, etc. should never enter your repository\nPull before pushing: Always integrate others’ changes before pushing your own\nUse meaningful branch names: Names like feature/user-login or fix/validation-bug explain the purpose\n\nVersion control may seem like an overhead for very small projects, but establishing these habits early will pay dividends as your projects grow in size and complexity. It’s much easier to start with good practices than to retrofit them later.\nIn the next section, we’ll set up a virtual environment and explore basic dependency management to isolate your project and manage its requirements."
  },
  {
    "objectID": "chapters/01-foundation.html#virtual-environments-and-basic-dependencies",
    "href": "chapters/01-foundation.html#virtual-environments-and-basic-dependencies",
    "title": "2  Setting the Foundation",
    "section": "2.3 Virtual Environments and Basic Dependencies",
    "text": "2.3 Virtual Environments and Basic Dependencies\nPython’s flexibility with packages and imports is powerful, but can quickly lead to conflicts between projects. Virtual environments solve this problem by creating isolated spaces for each project’s dependencies.\n\n2.3.1 Understanding Virtual Environments\nA virtual environment is an isolated copy of Python with its own packages, separate from your system Python installation. This isolation ensures:\n\nDifferent projects can use different versions of the same package\nInstalling a package for one project won’t affect others\nYour development environment closely matches production\n\n\n\n2.3.2 Setting Up a Virtual Environment with venv\nPython comes with venv built in, making it the simplest way to create virtual environments:\n# Create a virtual environment named \".venv\" in your project\npython -m venv .venv\n\n# Activate the environment (the command differs by platform)\n# On Windows:\n.venv\\Scripts\\activate\n# On macOS/Linux:\nsource .venv/bin/activate\n\n# Your prompt should change to indicate the active environment\n(venv) $\nOnce activated, any packages you install will be confined to this environment. When you’re done working on the project, you can deactivate the environment:\ndeactivate\n\nTip: Using .venv as the environment name (with the leading dot) makes it hidden in many file browsers, reducing clutter. Make sure .venv/ is in your .gitignore file - you never want to commit this directory.\n\n\n\n2.3.3 Basic Dependency Management\nWith your virtual environment active, you can install packages using pip:\n# Install a specific package\npip install requests\n\n# Install multiple packages\npip install pytest black\nWhen working on a team project or deploying to production, you’ll need to track and share these dependencies. The simplest approach uses pip freeze:\n# Capture all installed packages and their versions\npip freeze &gt; requirements.txt\n\n# On another machine, install the exact same packages\npip install -r requirements.txt\nThis approach works well for simple projects, especially when you’re just getting started. However, as we’ll see in Part 2, there are limitations to this method:\n\nIt captures indirect dependencies (dependencies of your dependencies) which can make the file harder to maintain\nIt doesn’t distinguish between your project’s requirements and development tools\nIt can sometimes be too strict, pinning packages to versions that might not be necessary\n\n\nLooking Ahead: In Part 2, we’ll explore more robust dependency management with tools like pip-tools and uv, which solve these limitations by creating proper “lock files” while maintaining a clean list of direct dependencies. We’ll also see how these tools help ensure deterministic builds - a crucial feature as your projects grow in complexity.\n\n\n\n2.3.4 Practical Example: Setting Up a New Project\nLet’s combine what we’ve learned so far with a practical example. Here’s how to set up a new project with good practices:\n# Create project structure\nmkdir -p my_project/src/my_package my_project/tests\ncd my_project\n\n# Initialize Git repository\ngit init\necho \"*.pyc\\n__pycache__/\\n.venv/\\n*.egg-info/\" &gt; .gitignore\n\n# Create basic files\necho \"# My Project\\n\\nA description of my project.\" &gt; README.md\ntouch src/my_package/__init__.py\ntouch src/my_package/main.py\ntouch tests/__init__.py\ntouch tests/test_main.py\ntouch requirements.in\n\n# Create and activate virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install initial dependencies\npip install pytest\npip freeze &gt; requirements.txt\n\n# Initial Git commit\ngit add .\ngit commit -m \"Initial project setup\""
  },
  {
    "objectID": "chapters/01-foundation.html#jumpstarting-your-projects-with-templates",
    "href": "chapters/01-foundation.html#jumpstarting-your-projects-with-templates",
    "title": "2  Setting the Foundation",
    "section": "2.4 Jumpstarting Your Projects with Templates",
    "text": "2.4 Jumpstarting Your Projects with Templates\nNow that we’ve covered the essential foundation for Python development, you might be wondering how to apply these practices efficiently when starting new projects. Rather than recreating this structure manually each time, we offer two approaches to jumpstart your projects:\n\n2.4.1 Simple Scaffolding Script\nFor those who prefer a transparent, straightforward approach, we’ve created a simple bash script that creates the basic project structure we’ve discussed:\n# Download the script\ncurl -O https://example.com/scaffold_python_project.sh\nchmod +x scaffold_python_project.sh\n\n# Create a new project\n./scaffold_python_project.sh my_project\nThis script creates a minimal but well-structured Python project with: - The recommended src layout - Basic test setup - Simple pyproject.toml configuration - Version control initialization - Placeholder documentation\nThe script is intentionally simple and readable, allowing you to understand exactly what’s happening and modify it for your specific needs. This approach is ideal for learning or for smaller projects where you want maximum visibility into the setup process.\n\n\n2.4.2 Cookiecutter Template (For More Comprehensive Setup)\nFor more complex projects or when you want a more feature-rich starting point, we also provide a cookiecutter template that implements the full development pipeline described throughout this book:\n# Install cookiecutter\npip install cookiecutter\n\n# Create a new project from the template\ncookiecutter gh:username/python-dev-pipeline-cookiecutter\nThe cookiecutter template offers more customization options and includes: - All the foundational structure from the simple script - Comprehensive tool configurations - Optional documentation setup with MkDocs - CI/CD workflow configurations - Advanced dependency management - Security scanning integration\nThis approach is covered in detail in Appendix C and is recommended when you’re ready to adopt more advanced practices or when working with larger teams.\n\n\n2.4.3 GitHub Repository Templates (For No-Installation Simplicity)\nFor the ultimate in simplicity, we also provide a GitHub repository template that requires no local tool installation. GitHub templates offer a frictionless way to create new projects with the same structure and files:\n\nVisit the template repository at https://github.com/username/python-project-template\nClick the “Use this template” button\nName your new repository and create it\nClone your new repository locally\n\ngit clone https://github.com/yourusername/your-new-project.git\ncd your-new-project\nWhile GitHub templates don’t offer the same parameterization as cookiecutter (file contents remain exactly as they were in the template), they provide the lowest barrier to entry for getting started with a well-structured project. After creating your repository from the template, you can manually customize file contents like project name, author information, and other details.\nThe GitHub template includes: - The recommended src layout - Basic test structure - .gitignore and pyproject.toml configuration - Documentation structure - Example code and tests\nThis approach is ideal for quickly starting new projects when you don’t want to install additional tools or when you’re introducing others to Python best practices with minimal setup overhead.\nAll these options—the simple script, the cookiecutter template—embody, and GitHub repository templates embody our philosophy of “Simple but not Simplistic.” Choose the option that best fits your current needs and comfort level. As your projects grow in complexity, you can gradually adopt more sophisticated practices while maintaining the solid foundation established here.\nIn Part 2, we’ll build on this foundation by exploring robust dependency management, code quality tools, testing strategies, and type checking—the next layers in our Python development pipeline."
  },
  {
    "objectID": "chapters/02-workflow.html#robust-dependency-management-with-pip-tools-and-uv",
    "href": "chapters/02-workflow.html#robust-dependency-management-with-pip-tools-and-uv",
    "title": "3  Advancing Your Workflow",
    "section": "3.1 Robust Dependency Management with pip-tools and uv",
    "text": "3.1 Robust Dependency Management with pip-tools and uv\nAs your projects grow in complexity or involve more developers, the basic pip freeze &gt; requirements.txt approach starts to show limitations. You need a dependency management system that gives you more control and ensures truly reproducible environments.\n\n3.1.1 The Problem with pip freeze\nWhile pip freeze is convenient, it has several drawbacks:\n\nNo distinction between direct and indirect dependencies: You can’t easily tell which packages you explicitly need versus those that were installed as dependencies of other packages.\nMaintenance challenges: When you want to update a package, you may need to regenerate the entire requirements file, potentially changing packages you didn’t intend to update.\nNo environment synchronization: Installing from a requirements.txt file adds packages but doesn’t remove packages that are no longer needed.\nNo explicit dependency specification: You can’t easily specify version ranges (e.g., “I need any Django 4.x version”) or extras.\n\nLet’s explore two powerful solutions: pip-tools and uv.\n\n\n3.1.2 Solution 1: pip-tools\npip-tools introduces a two-file approach to dependency management:\n\nrequirements.in: A manually maintained list of your direct dependencies, potentially with version constraints.\nrequirements.txt: A generated lock file containing exact versions of all dependencies (direct and indirect).\n\n\n3.1.2.1 Getting Started with pip-tools\n# Install pip-tools in your virtual environment\npip install pip-tools\n\n# Create a requirements.in file with your direct dependencies\ncat &gt; requirements.in &lt;&lt; EOF\nrequests&gt;=2.25.0  # Use any version 2.25.0 or newer\nflask==2.0.1      # Use exactly this version\npandas            # Use any version\nEOF\n\n# Compile the lock file\npip-compile requirements.in\n\n# Install the exact dependencies\npip-sync requirements.txt\nThe generated requirements.txt will contain exact versions of your specified packages plus all their dependencies, including hashes for security.\n\n\n3.1.2.2 Managing Development Dependencies\nFor a cleaner setup, you can separate production and development dependencies:\n# Create requirements-dev.in\ncat &gt; requirements-dev.in &lt;&lt; EOF\n-c requirements.txt  # Constraint: use same versions as in requirements.txt\npytest&gt;=7.0.0\npytest-cov\nruff\nmypy\nEOF\n\n# Compile development dependencies\npip-compile requirements-dev.in -o requirements-dev.txt\n\n# Install all dependencies (both prod and dev)\npip-sync requirements.txt requirements-dev.txt\n\n\n3.1.2.3 Updating Dependencies\nWhen you need to update packages:\n# Update all packages to their latest allowed versions\npip-compile --upgrade requirements.in\n\n# Update a specific package\npip-compile --upgrade-package requests requirements.in\n\n# After updating, sync your environment\npip-sync requirements.txt\n\n\n\n3.1.3 Solution 2: uv\nuv is a newer, Rust-based tool that provides significant speed improvements while maintaining compatibility with existing Python packaging standards. It combines environment management, package installation, and dependency resolution in one tool.\n\n3.1.3.1 Getting Started with uv\n# Install uv (globally with pipx or in your current environment)\npipx install uv\n# Or: pip install uv\n\n# Create a virtual environment (if needed)\nuv venv\n\n# Activate the environment as usual\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Create the same requirements.in file as above\ncat &gt; requirements.in &lt;&lt; EOF\nrequests&gt;=2.25.0\nflask==2.0.1\npandas\nEOF\n\n# Compile the lock file\nuv pip compile requirements.in -o requirements.txt\n\n# Install dependencies\nuv pip sync requirements.txt\n\n\n3.1.3.2 Key Advantages of uv\n\nSpeed: uv is significantly faster than standard pip and pip-tools, especially for large dependency trees.\nGlobal caching: uv implements efficient caching, reducing redundant downloads across projects.\nConsolidated tooling: Acts as a replacement for multiple tools (pip, pip-tools, virtualenv) with a consistent interface.\nEnhanced dependency resolution: Often provides clearer error messages for dependency conflicts.\n\n\n\n3.1.3.3 Managing Dependencies with uv\nuv supports the same workflow as pip-tools but with different commands:\n# For development dependencies\ncat &gt; requirements-dev.in &lt;&lt; EOF\n-c requirements.txt\npytest&gt;=7.0.0\npytest-cov\nruff\nmypy\nEOF\n\n# Compile dev dependencies\nuv pip compile requirements-dev.in -o requirements-dev.txt\n\n# Install all dependencies\nuv pip sync requirements.txt requirements-dev.txt\n\n# Update a specific package\nuv pip compile --upgrade-package requests requirements.in\n\n\n\n3.1.4 Choosing Between pip-tools and uv\nBoth tools solve the core problem of creating reproducible environments, but with different tradeoffs:\n\n\n\n\n\n\n\n\nFactor\npip-tools\nuv\n\n\n\n\nSpeed\nGood\nExcellent (often 10x+ faster)\n\n\nInstallation\nSimple Python package\nExternal tool (but simple to install)\n\n\nMaturity\nWell-established\nNewer but rapidly maturing\n\n\nFunctionality\nFocused on dependency locking\nBroader tool combining multiple functions\n\n\nLearning curve\nMinimal\nMinimal (designed for compatibility)\n\n\n\nFor beginners or smaller projects, pip-tools offers a gentle introduction to proper dependency management with minimal new concepts. For larger projects or when speed becomes important, uv provides significant benefits with a similar workflow.\n\n\n3.1.5 Best Practices for Either Approach\nRegardless of which tool you choose:\n\nCommit both .in and .txt files to version control. The .in files represent your intent, while the .txt files ensure reproducibility.\nUse constraints carefully. Start with loose constraints (just package names) and add version constraints only when needed.\nRegularly update dependencies to get security fixes, using --upgrade or --upgrade-package.\nAlways use pip-sync or uv pip sync instead of pip install -r requirements.txt to ensure your environment exactly matches the lock file.\n\nIn the next section, we’ll explore how to maintain code quality through automated formatting and linting with Ruff, taking your workflow to the next professional level."
  },
  {
    "objectID": "chapters/02-workflow.html#code-quality-tools-with-ruff",
    "href": "chapters/02-workflow.html#code-quality-tools-with-ruff",
    "title": "3  Advancing Your Workflow",
    "section": "3.2 Code Quality Tools with Ruff",
    "text": "3.2 Code Quality Tools with Ruff\nWriting code that works is only part of the development process. Code should also be readable, maintainable, and free from common errors. This is where code quality tools come in, helping you enforce consistent style and catch potential issues early.\n\n3.2.1 The Evolution of Python Code Quality Tools\nTraditionally, Python developers used multiple specialized tools:\n\nBlack for code formatting\nisort for import sorting\nFlake8 for linting (style checks)\nPylint for deeper static analysis\n\nWhile effective, maintaining configuration for all these tools was cumbersome. Enter Ruff – a modern, Rust-based tool that combines formatting and linting in one incredibly fast package.\n\n\n3.2.2 Why Ruff?\nRuff offers several compelling advantages:\n\nSpeed: Often 10-100x faster than traditional Python linters\nConsolidation: Replaces multiple tools with one consistent interface\nCompatibility: Implements rules from established tools (Flake8, Black, isort, etc.)\nConfiguration: Single configuration in your pyproject.toml file\nAutomatic fixing: Can automatically fix many issues it identifies\n\n\n\n3.2.3 Getting Started with Ruff\nFirst, install Ruff in your virtual environment:\n# If using pip\npip install ruff\n\n# If using uv\nuv pip install ruff\n\n\n3.2.4 Basic Configuration\nConfigure Ruff in your pyproject.toml file:\n[tool.ruff]\n# Enable pycodestyle, Pyflakes, isort, and more\nselect = [\"E\", \"F\", \"I\"]\nignore = []\n\n# Allow lines to be as long as 100 characters\nline-length = 100\n\n# Assume Python 3.10\ntarget-version = \"py310\"\n\n[tool.ruff.format]\n# Formats code similar to Black (this is the default)\nquote-style = \"double\"\nindent-style = \"space\"\nline-ending = \"auto\"\nThis configuration enables: - E rules from pycodestyle (PEP 8 style guide) - F rules from Pyflakes (logical and syntax error detection) - I rules for import sorting (like isort)\n\n\n3.2.5 Using Ruff in Your Workflow\nRuff provides two main commands:\n# Check code for issues without changing it\nruff check .\n\n# Format code (similar to Black)\nruff format .\nTo automatically fix issues that Ruff can solve:\n# Fix all auto-fixable issues\nruff check --fix .\n\n\n3.2.6 Real-world Configuration Example\nHere’s a more comprehensive configuration that balances strictness with practicality:\n[tool.ruff]\n# Target Python version\ntarget-version = \"py39\"\n# Line length\nline-length = 88\n\n# Enable a comprehensive set of rules\nselect = [\n    \"E\",   # pycodestyle errors\n    \"F\",   # pyflakes\n    \"I\",   # isort\n    \"W\",   # pycodestyle warnings\n    \"C90\", # mccabe complexity\n    \"N\",   # pep8-naming\n    \"B\",   # flake8-bugbear\n    \"UP\",  # pyupgrade\n    \"D\",   # pydocstyle\n]\n\n# Ignore specific rules\nignore = [\n    \"E203\",  # Whitespace before ':' (handled by formatter)\n    \"D100\",  # Missing docstring in public module\n    \"D104\",  # Missing docstring in public package\n]\n\n# Exclude certain files/directories from checking\nexclude = [\n    \".git\",\n    \".venv\",\n    \"__pycache__\",\n    \"build\",\n    \"dist\",\n]\n\n[tool.ruff.pydocstyle]\n# Use Google-style docstrings\nconvention = \"google\"\n\n[tool.ruff.mccabe]\n# Maximum McCabe complexity allowed\nmax-complexity = 10\n\n[tool.ruff.format]\n# Formatting options (black-compatible by default)\nquote-style = \"double\"\n\n\n3.2.7 Integrating Ruff into Your Editor\nRuff provides editor integrations for:\n\nVS Code (via the Ruff extension)\nPyCharm (via third-party plugin)\nVim/Neovim\nEmacs\n\nFor example, in VS Code, install the Ruff extension and add to your settings.json:\n{\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n        \"source.fixAll.ruff\": true,\n        \"source.organizeImports.ruff\": true\n    }\n}\nThis configuration automatically formats code and fixes issues whenever you save a file.\n\n\n3.2.8 Gradually Adopting Ruff\nIf you’re working with an existing codebase, you can adopt Ruff gradually:\n\nStart with formatting only: Begin with ruff format to establish consistent formatting\nAdd basic linting: Enable a few rule sets like E, F, and I\nGradually increase strictness: Add more rule sets as your team adjusts\nUse per-file ignores: For specific issues in specific files\n\n[tool.ruff.per-file-ignores]\n\"tests/*\" = [\"D103\"]  # Ignore missing docstrings in tests\n\"__init__.py\" = [\"F401\"]  # Ignore unused imports in __init__.py\n\n\n3.2.9 Enforcing Code Quality in CI\nAdd Ruff to your CI pipeline to ensure code quality standards are maintained:\n# In your GitHub Actions workflow (.github/workflows/ci.yml)\n- name: Check formatting with Ruff\n  run: ruff format --check .\n  \n- name: Lint with Ruff\n  run: ruff check .\nThe --check flag on ruff format makes it exit with an error if files would be reformatted, instead of actually changing them.\n\n\n3.2.10 Beyond Ruff: When to Consider Other Tools\nWhile Ruff covers a wide range of code quality checks, some specific needs might require additional tools:\n\nmypy for static type checking (covered in a later section)\nbandit for security-focused checks\nvulture for finding dead code\n\nHowever, Ruff’s rule set continues to expand, potentially reducing the need for these additional tools over time.\nBy incorporating Ruff into your workflow, you’ll catch many common errors before they reach production and maintain a consistent, readable codebase. In the next section, we’ll explore how to ensure your code works as expected through automated testing with pytest."
  },
  {
    "objectID": "chapters/02-workflow.html#automated-testing-with-pytest",
    "href": "chapters/02-workflow.html#automated-testing-with-pytest",
    "title": "3  Advancing Your Workflow",
    "section": "3.3 Automated Testing with pytest",
    "text": "3.3 Automated Testing with pytest\nTesting is a crucial aspect of software development that ensures your code works as intended and continues to work as you make changes. Python’s testing ecosystem offers numerous frameworks, but pytest has emerged as the most popular and powerful choice for most projects.\n\n3.3.1 Why Testing Matters\nAutomated tests provide several key benefits:\n\nVerification: Confirm that your code works as expected\nRegression prevention: Catch when changes break existing functionality\nDocumentation: Tests demonstrate how code is meant to be used\nRefactoring confidence: Change code structure while ensuring behavior remains correct\nDesign feedback: Difficult-to-test code often indicates design problems\n\n\n\n3.3.2 Getting Started with pytest\nFirst, install pytest in your virtual environment:\n# Standard installation\npip install pytest\n\n# With coverage reporting\npip install pytest pytest-cov\n\n\n3.3.3 Writing Your First Test\nLet’s assume you have a simple function in src/my_package/calculations.py:\ndef add(a, b):\n    \"\"\"Add two numbers and return the result.\"\"\"\n    return a + b\nCreate a test file in tests/test_calculations.py:\nfrom my_package.calculations import add\n\ndef test_add():\n    # Test basic addition\n    assert add(1, 2) == 3\n    \n    # Test with negative numbers\n    assert add(-1, 1) == 0\n    assert add(-1, -1) == -2\n    \n    # Test with floating point\n    assert add(1.5, 2.5) == 4.0\n\n\n3.3.4 Running Tests\nRun all tests from your project root:\n# Run all tests\npytest\n\n# Run with more detail\npytest -v\n\n# Run a specific test file\npytest tests/test_calculations.py\n\n# Run a specific test function\npytest tests/test_calculations.py::test_add\n\n\n3.3.5 pytest Features That Make Testing Easier\npytest has several features that make it superior to Python’s built-in unittest framework:\n\n3.3.5.1 1. Simple Assertions\nInstead of methods like assertEqual or assertTrue, pytest lets you use Python’s built-in assert statement, making tests more readable.\n# With pytest\nassert result == expected\n\n# Instead of unittest's\nself.assertEqual(result, expected)\n\n\n3.3.5.2 2. Fixtures\nFixtures are a powerful way to set up preconditions for your tests:\nimport pytest\nfrom my_package.database import Database\n\n@pytest.fixture\ndef db():\n    \"\"\"Provide a clean database instance for tests.\"\"\"\n    db = Database(\":memory:\")  # Use in-memory SQLite\n    db.create_tables()\n    yield db\n    db.close()  # Cleanup happens after the test\n\ndef test_save_record(db):\n    # The db fixture is automatically provided\n    record = {\"id\": 1, \"name\": \"Test\"}\n    db.save(record)\n    assert db.get(1) == record\n\n\n3.3.5.3 3. Parameterized Tests\nTest multiple inputs without repetitive code:\nimport pytest\nfrom my_package.calculations import add\n\n@pytest.mark.parametrize(\"a, b, expected\", [\n    (1, 2, 3),\n    (-1, 1, 0),\n    (0, 0, 0),\n    (1.5, 2.5, 4.0),\n])\ndef test_add_parametrized(a, b, expected):\n    assert add(a, b) == expected\n\n\n3.3.5.4 4. Marks for Test Organization\nOrganize tests with marks:\n@pytest.mark.slow\ndef test_complex_calculation():\n    # This test takes a long time\n    ...\n\n# Run only tests marked as 'slow'\n# pytest -m slow\n\n@pytest.mark.skip(reason=\"Feature not implemented yet\")\ndef test_future_feature():\n    ...\n\n@pytest.mark.xfail(reason=\"Known bug #123\")\ndef test_buggy_function():\n    ...\n\n\n\n3.3.6 Test Coverage\nTrack which parts of your code are tested using pytest-cov:\n# Run tests with coverage report\npytest --cov=src/my_package\n\n# Generate HTML report for detailed analysis\npytest --cov=src/my_package --cov-report=html\n# Then open htmlcov/index.html in your browser\nA coverage report helps identify untested code:\n----------- coverage: platform linux, python 3.9.5-final-0 -----------\nName                             Stmts   Miss  Cover\n----------------------------------------------------\nsrc/my_package/__init__.py           1      0   100%\nsrc/my_package/calculations.py      10      2    80%\nsrc/my_package/models.py            45     15    67%\n----------------------------------------------------\nTOTAL                               56     17    70%\n\n\n3.3.7 Testing Best Practices\n\nWrite tests as you develop: Don’t wait until the end\nName tests clearly: Include the function name and scenario being tested\nOne assertion per test: Focus each test on a single behavior\nTest edge cases: Empty input, boundary values, error conditions\nAvoid test interdependence: Tests should work independently\nMock external dependencies: APIs, databases, file systems\nKeep tests fast: Slow tests get run less often\n\n\n\n3.3.8 Common Testing Patterns\n\n3.3.8.1 Testing Exceptions\nVerify that your code raises the right exceptions:\nimport pytest\nfrom my_package.validate import validate_username\n\ndef test_validate_username_too_short():\n    with pytest.raises(ValueError) as excinfo:\n        validate_username(\"ab\")  # Too short\n    assert \"Username must be at least 3 characters\" in str(excinfo.value)\n\n\n3.3.8.2 Testing with Temporary Files\nTest file operations safely:\ndef test_save_to_file(tmp_path):\n    # tmp_path is a built-in pytest fixture\n    file_path = tmp_path / \"test.txt\"\n    \n    # Test file writing\n    save_to_file(file_path, \"test content\")\n    \n    # Verify content\n    assert file_path.read_text() == \"test content\"\n\n\n3.3.8.3 Mocking\nIsolate your code from external dependencies using the pytest-mock plugin:\ndef test_fetch_user_data(mocker):\n    # Mock the API call\n    mock_response = mocker.patch('requests.get')\n    mock_response.return_value.json.return_value = {\"id\": 1, \"name\": \"Test User\"}\n    \n    # Test our function\n    from my_package.api import get_user\n    user = get_user(1)\n    \n    # Verify results\n    assert user['name'] == \"Test User\"\n    mock_response.assert_called_once_with('https://api.example.com/users/1')\n\n\n\n3.3.9 Testing Strategy\nAs your project grows, organize tests into different categories:\n\nUnit tests: Test individual functions/classes in isolation\nIntegration tests: Test interactions between components\nFunctional tests: Test entire features from a user perspective\n\nMost projects should have a pyramid shape: many unit tests, fewer integration tests, and even fewer functional tests.\n\n\n3.3.10 Continuous Testing\nMake testing a habitual part of your workflow:\n\nRun relevant tests as you code: Many editors integrate with pytest\nRun full test suite before committing: Use pre-commit hooks\nRun tests in CI: Catch issues that might only appear in different environments\n\nBy incorporating comprehensive testing into your development process, you’ll catch bugs earlier, ship with more confidence, and build a more maintainable codebase.\nIn the next section, we’ll explore static type checking with mypy, which can help catch a whole new category of errors before your code even runs."
  },
  {
    "objectID": "chapters/02-workflow.html#type-checking-with-mypy",
    "href": "chapters/02-workflow.html#type-checking-with-mypy",
    "title": "3  Advancing Your Workflow",
    "section": "3.4 Type Checking with mypy",
    "text": "3.4 Type Checking with mypy\nPython is dynamically typed, which provides flexibility but can also lead to type-related errors that only appear at runtime. Static type checking with mypy adds an extra layer of verification, catching many potential issues before your code executes.\n\n3.4.1 Understanding Type Hints\nPython 3.5+ supports type hints, which are annotations indicating what types of values functions expect and return:\ndef greeting(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\nThese annotations don’t change how Python runs—they’re ignored by the interpreter at runtime. However, tools like mypy can analyze them statically to catch potential type errors.\n\n\n3.4.2 Getting Started with mypy\nFirst, install mypy in your development environment:\npip install mypy\nLet’s check a simple example:\n# example.py\ndef double(x: int) -&gt; int:\n    return x * 2\n\n# This is fine\nresult = double(5)\n\n# This would fail at runtime\ndouble(\"hello\")\nRun mypy to check:\nmypy example.py\nOutput:\nexample.py:8: error: Argument 1 to \"double\" has incompatible type \"str\"; expected \"int\"\nmypy caught the type mismatch without running the code!\n\n\n3.4.3 Configuring mypy\nConfigure mypy in your pyproject.toml file for a consistent experience:\n[tool.mypy]\npython_version = \"3.9\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = false\ndisallow_incomplete_defs = false\nStart with a lenient configuration and gradually increase strictness:\n# Starting configuration: permissive but helpful\n[tool.mypy]\npython_version = \"3.9\"\nwarn_return_any = true\ncheck_untyped_defs = true\ndisallow_untyped_defs = false\n\n# Intermediate configuration: more rigorous\n[tool.mypy]\npython_version = \"3.9\"\nwarn_return_any = true\ndisallow_incomplete_defs = true\ndisallow_untyped_defs = false\ncheck_untyped_defs = true\n\n# Strict configuration: full typing required\n[tool.mypy]\npython_version = \"3.9\"\ndisallow_untyped_defs = true\ndisallow_incomplete_defs = true\nno_implicit_optional = true\nwarn_redundant_casts = true\nwarn_unused_ignores = true\nwarn_return_any = true\nwarn_unreachable = true\n\n\n3.4.4 Gradual Typing\nOne major advantage of Python’s type system is gradual typing—you can add types incrementally:\n\nStart with critical or error-prone modules\nAdd types to public interfaces first\nIncrease type coverage over time\n\n\n\n3.4.5 Essential Type Annotations\n\n3.4.5.1 Basic Types\n# Variables\nname: str = \"Alice\"\nage: int = 30\nheight: float = 1.75\nis_active: bool = True\n\n# Lists, sets, and dictionaries\nnames: list[str] = [\"Alice\", \"Bob\"]\nunique_ids: set[int] = {1, 2, 3}\nuser_scores: dict[str, int] = {\"Alice\": 100, \"Bob\": 85}\n\n\n3.4.5.2 Function Annotations\ndef calculate_total(prices: list[float], tax_rate: float = 0.0) -&gt; float:\n    \"\"\"Calculate the total price including tax.\"\"\"\n    subtotal = sum(prices)\n    return subtotal * (1 + tax_rate)\n\n\n3.4.5.3 Class Annotations\nfrom typing import Optional\n\nclass User:\n    def __init__(self, name: str, email: str, age: Optional[int] = None):\n        self.name: str = name\n        self.email: str = email\n        self.age: Optional[int] = age\n    \n    def is_adult(self) -&gt; bool:\n        \"\"\"Check if user is an adult.\"\"\"\n        return self.age is not None and self.age &gt;= 18\n\n\n\n3.4.6 Advanced Type Hints\n\n3.4.6.1 Union Types\nUse Union to indicate multiple possible types (use the | operator in Python 3.10+):\nfrom typing import Union\n\n# Python 3.9 and earlier\ndef process_input(data: Union[str, list[str]]) -&gt; str:\n    if isinstance(data, list):\n        return \", \".join(data)\n    return data\n\n# Python 3.10+\ndef process_input(data: str | list[str]) -&gt; str:\n    if isinstance(data, list):\n        return \", \".join(data)\n    return data\n\n\n3.4.6.2 Optional and None\nOptional[T] is equivalent to Union[T, None] or T | None:\nfrom typing import Optional\n\ndef find_user(user_id: int) -&gt; Optional[dict]:\n    \"\"\"Return user data or None if not found.\"\"\"\n    # Implementation...\n\n\n3.4.6.3 Type Aliases\nCreate aliases for complex types:\nfrom typing import Dict, List, Tuple\n\n# Complex type\nTransactionRecord = Tuple[str, float, str, Dict[str, str]]\n\n# More readable with alias\ndef process_transactions(transactions: List[TransactionRecord]) -&gt; float:\n    total = 0.0\n    for _, amount, _, _ in transactions:\n        total += amount\n    return total\n\n\n3.4.6.4 Callable\nType hint for functions:\nfrom typing import Callable\n\ndef apply_function(func: Callable[[int], str], value: int) -&gt; str:\n    \"\"\"Apply a function that converts int to str.\"\"\"\n    return func(value)\n\n\n\n3.4.7 Common Challenges and Solutions\n\n3.4.7.1 Working with Third-Party Libraries\nNot all libraries provide type hints. For popular packages, you can often find stub files:\npip install types-requests\nFor others, you can silence mypy warnings selectively:\nimport untyped_library  # type: ignore\n\n\n3.4.7.2 Dealing with Dynamic Features\nPython’s dynamic features can be challenging to type. Use Any when necessary:\nfrom typing import Any, Dict\n\ndef parse_config(config: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Parse configuration with unknown structure.\"\"\"\n    # Implementation...\n\n\n\n3.4.8 Integration with Your Workflow\n\n3.4.8.1 Running mypy\n# Check a specific file\nmypy src/my_package/module.py\n\n# Check the entire package\nmypy src/my_package/\n\n# Use multiple processes for faster checking\nmypy -p my_package --python-version 3.9 --multiprocessing\n\n\n3.4.8.2 Integrating with CI/CD\nAdd mypy to your continuous integration workflow:\n# GitHub Actions example\n- name: Type check with mypy\n  run: mypy src/\n\n\n3.4.8.3 Editor Integration\nMost Python-friendly editors support mypy:\n\nVS Code: Use the Pylance extension\nPyCharm: Has built-in type checking\nvim/neovim: Use ALE or similar plugins\n\n\n\n\n3.4.9 Benefits of Type Checking\n\nCatch errors early: Find type-related bugs before running code\nImproved IDE experience: Better code completion and refactoring\nSelf-documenting code: Types serve as documentation\nSafer refactoring: Change code with more confidence\nGradual adoption: Add types where they provide the most value\n\n\n\n3.4.10 When to Use Type Hints\nType hints are particularly valuable for:\n\nFunctions with complex parameters or return values\nPublic APIs used by others\nAreas with frequent bugs\nCritical code paths\nLarge codebases with multiple contributors\n\nType checking isn’t an all-or-nothing proposition. Even partial type coverage can significantly improve code quality and catch common errors. Start small, focus on interfaces, and expand your type coverage as your team becomes comfortable with the system."
  },
  {
    "objectID": "chapters/02-workflow.html#security-analysis-with-bandit",
    "href": "chapters/02-workflow.html#security-analysis-with-bandit",
    "title": "3  Advancing Your Workflow",
    "section": "3.5 Security Analysis with Bandit",
    "text": "3.5 Security Analysis with Bandit\nSoftware security is a critical concern in modern development, yet it’s often overlooked until problems arise. Bandit is a tool designed to find common security issues in Python code through static analysis.\n\n3.5.1 Understanding Security Static Analysis\nUnlike functional testing or linting, security-focused static analysis looks specifically for patterns and practices that could lead to security vulnerabilities:\n\nInjection vulnerabilities\nUse of insecure functions\nHardcoded credentials\nInsecure cryptography\nAnd many other security issues\n\n\n\n3.5.2 Getting Started with Bandit\nFirst, install Bandit in your virtual environment:\npip install bandit\nRun a basic scan:\n# Scan a specific file\nbandit -r src/my_package/main.py\n\n# Scan your entire codebase\nbandit -r src/\n\n\n3.5.3 Security Issues Bandit Can Detect\nBandit identifies a wide range of security concerns, including:\n\n3.5.3.1 1. Hardcoded Secrets\n# Bandit will flag this\ndef connect_to_database():\n    password = \"super_secret_password\"  # Hardcoded secret\n    return Database(\"user\", password)\n\n\n3.5.3.2 2. SQL Injection\n# Vulnerable to SQL injection\ndef get_user(username):\n    query = f\"SELECT * FROM users WHERE username = '{username}'\"\n    return db.execute(query)\n\n# Safer approach\ndef get_user_safe(username):\n    query = \"SELECT * FROM users WHERE username = %s\"\n    return db.execute(query, (username,))\n\n\n3.5.3.3 3. Shell Injection\n# Vulnerable to command injection\ndef run_command(user_input):\n    return os.system(f\"ls {user_input}\")  # User could inject commands\n\n# Safer approach\nimport subprocess\ndef run_command_safe(user_input):\n    return subprocess.run([\"ls\", user_input], capture_output=True, text=True)\n\n\n3.5.3.4 4. Insecure Cryptography\n# Using weak hash algorithms\nimport hashlib\ndef hash_password(password):\n    return hashlib.md5(password.encode()).hexdigest()  # MD5 is insecure\n\n\n3.5.3.5 5. Unsafe Deserialization\n# Insecure deserialization\nimport pickle\ndef load_user_preferences(data):\n    return pickle.loads(data)  # Pickle can execute arbitrary code\n\n\n\n3.5.4 Configuring Bandit\nYou can configure Bandit using a .bandit file or your pyproject.toml:\n[tool.bandit]\nexclude_dirs = [\"tests\", \"docs\"]\nskips = [\"B311\"]  # Skip random warning\ntargets = [\"src\"]\nThe most critical findings are categorized with high severity and confidence levels:\n# Only report high-severity issues\nbandit -r src/ -iii -ll\n\n\n3.5.5 Integrating Bandit in Your Workflow\n\n3.5.5.1 Add Bandit to CI/CD\nAdd security scanning to your continuous integration pipeline:\n# GitHub Actions example\n- name: Security check with Bandit\n  run: bandit -r src/ -f json -o bandit-results.json\n\n# Optional: convert results to GitHub Security format\n# (requires additional tools or post-processing)\n\n\n3.5.5.2 Pre-commit Hook\nConfigure a pre-commit hook to run Bandit before commits:\n# In .pre-commit-config.yaml\n- repo: https://github.com/PyCQA/bandit\n  rev: 1.7.5\n  hooks:\n    - id: bandit\n      args: [\"-r\", \"src\"]\n\n\n\n3.5.6 Responding to Security Findings\nWhen Bandit identifies security issues:\n\nUnderstand the risk: Read the detailed explanation to understand the potential vulnerability\nFix high-severity issues immediately: These represent significant security risks\nDocument deliberate exceptions: If a finding is a false positive, document why and use an inline ignore comment\nReview regularly: Security standards evolve, so regular scanning is essential\n\n\n\n3.5.7 False Positives\nLike any static analysis tool, Bandit can produce false positives. You can exclude specific findings:\n# In code, to ignore a specific line\nimport pickle  # nosec\n\n# For a whole file\n# nosec\n\n# Or configure globally in pyproject.toml\nBy incorporating security scanning with Bandit, you add an essential layer of protection against common security vulnerabilities, helping to ensure that your code is not just functional but also secure."
  },
  {
    "objectID": "chapters/02-workflow.html#finding-dead-code-with-vulture",
    "href": "chapters/02-workflow.html#finding-dead-code-with-vulture",
    "title": "3  Advancing Your Workflow",
    "section": "3.6 Finding Dead Code with Vulture",
    "text": "3.6 Finding Dead Code with Vulture\nAs projects evolve, code can become obsolete but remain in the codebase, creating maintenance burdens and confusion. Vulture is a static analysis tool that identifies unused code – functions, classes, and variables that are defined but never used.\n\n3.6.1 The Problem of Dead Code\nDead code creates several issues:\n\nMaintenance overhead: Every line of code needs maintenance\nCognitive load: Developers need to understand code that serves no purpose\nFalse security: Tests might pass while dead code goes unchecked\nMisleading documentation: Dead code can appear in documentation generators\n\n\n\n3.6.2 Getting Started with Vulture\nInstall Vulture in your virtual environment:\npip install vulture\nRun a basic scan:\n# Scan a specific file\nvulture src/my_package/main.py\n\n# Scan your entire codebase\nvulture src/\n\n\n3.6.3 What Vulture Detects\nVulture identifies:\n\n3.6.3.1 1. Unused Variables\ndef process_data(data):\n    result = []  # Defined but never used\n    for item in data:\n        processed = transform(item)  # Unused variable\n        data.append(item * 2)\n    return data\n\n\n3.6.3.2 2. Unused Functions\ndef calculate_average(numbers):\n    \"\"\"Calculate the average of a list of numbers.\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\n# If this function is never called anywhere, Vulture will flag it\n\n\n3.6.3.3 3. Unused Classes\nclass LegacyFormatter:\n    \"\"\"Format data using the legacy method.\"\"\"\n    def __init__(self, data):\n        self.data = data\n        \n    def format(self):\n        return json.dumps(self.data)\n\n# If this class is never instantiated, Vulture will flag it\n\n\n3.6.3.4 4. Unused Imports\nimport os\nimport sys  # If sys is imported but never used\nimport json\nfrom datetime import datetime, timedelta  # If timedelta is never used\n\n\n\n3.6.4 Handling False Positives\nVulture can sometimes flag code that’s actually used but in ways it can’t detect. Common cases include:\n\nClasses used through reflection\nFunctions called in templates\nCode used in an importable public API\n\nYou can create a whitelist file to suppress these reports:\n# whitelist.py\n# unused_function  # vulture:ignore\nRun Vulture with the whitelist:\nvulture src/ whitelist.py\n\n\n3.6.5 Configuration and Integration\nAdd Vulture to your workflow:\n\n3.6.5.1 Command Line Options\n# Set minimum confidence (default is 60%)\nvulture --min-confidence 80 src/\n\n# Exclude test files\nvulture src/ --exclude \"test_*.py\"\n\n\n3.6.5.2 CI Integration\n# GitHub Actions example\n- name: Find dead code with Vulture\n  run: vulture src/ --min-confidence 80\n\n\n\n3.6.6 Best Practices for Dead Code Removal\n\nVerify before removing: Confirm the code is truly unused\nUse version control: Remove code through proper commits with explanations\nUpdate documentation: Ensure documentation reflects the changes\nRun tests: Confirm nothing breaks when the code is removed\nLook for patterns: Clusters of dead code often indicate larger architectural issues\n\n\n\n3.6.7 When to Run Vulture\n\nBefore major refactoring\nDuring codebase cleanup\nAs part of regular maintenance\nWhen preparing for a significant release\nWhen onboarding new team members (helps them focus on what matters)\n\nRegularly checking for and removing dead code keeps your codebase lean and maintainable. It also provides insights into how your application has evolved and may highlight areas where design improvements could be made.\nWith these additional security and code quality tools in place, your Python development workflow is now even more robust. Let’s move on to Part 3, where we’ll explore documentation and deployment options."
  },
  {
    "objectID": "chapters/03-documentation.html#documentation-options-from-pydoc-to-mkdocs",
    "href": "chapters/03-documentation.html#documentation-options-from-pydoc-to-mkdocs",
    "title": "4  Documentation and Deployment",
    "section": "4.1 Documentation Options: From pydoc to MkDocs",
    "text": "4.1 Documentation Options: From pydoc to MkDocs\nDocumentation is often neglected in software development, yet it’s crucial for ensuring others (including your future self) can understand and use your code effectively. Python offers a spectrum of documentation options, from simple built-in tools to sophisticated documentation generators.\n\n4.1.1 Starting Simple with Docstrings\nThe foundation of Python documentation is the humble docstring - a string literal that appears as the first statement in a module, function, class, or method:\ndef calculate_discount(price: float, discount_percent: float) -&gt; float:\n    \"\"\"Calculate the discounted price.\n    \n    Args:\n        price: The original price\n        discount_percent: The discount percentage (0-100)\n        \n    Returns:\n        The price after discount\n        \n    Raises:\n        ValueError: If discount_percent is negative or greater than 100\n    \"\"\"\n    if not 0 &lt;= discount_percent &lt;= 100:\n        raise ValueError(\"Discount percentage must be between 0 and 100\")\n    \n    discount = price * (discount_percent / 100)\n    return price - discount\nDocstrings become particularly useful when following a consistent format. Common conventions include:\n\nGoogle style (shown above)\nNumPy style (similar to Google style but with different section headers)\nreStructuredText (used by Sphinx)\n\n\n\n4.1.2 Viewing Documentation with pydoc\nPython’s built-in pydoc module provides a simple way to access documentation:\n# View module documentation in the terminal\npython -m pydoc my_package.module\n\n# Start an HTTP server to browse documentation\npython -m pydoc -b\nYou can also generate basic HTML documentation:\n# Create HTML for a specific module\npython -m pydoc -w my_package.module\n\n# Create HTML for an entire package\nmkdir -p docs/html\npython -m pydoc -w my_package\nmv my_package*.html docs/html/\nWhile simple, this approach has limitations: - Minimal styling - No cross-linking between documents - Limited navigation options\nFor beginner projects, however, it provides a fast way to make documentation available with zero dependencies.\n\n\n4.1.3 Simple Script for Basic Documentation Site\nFor slightly more organized documentation than plain pydoc, you can create a simple script that: 1. Generates pydoc HTML for all modules 2. Creates a basic index.html linking to them\nHere’s a minimal example script (build_docs.py):\nimport os\nimport importlib\nimport pkgutil\nimport pydoc\n\ndef generate_docs(package_name, output_dir=\"docs/api\"):\n    \"\"\"Generate HTML documentation for all modules in a package.\"\"\"\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Import the package\n    package = importlib.import_module(package_name)\n    \n    # Track all modules for index page\n    modules = []\n    \n    # Walk through all modules in the package\n    for _, modname, ispkg in pkgutil.walk_packages(package.__path__, package_name + '.'):\n        try:\n            # Generate HTML documentation\n            html_path = os.path.join(output_dir, modname + '.html')\n            with open(html_path, 'w') as f:\n                pydoc_output = pydoc.HTMLDoc().document(importlib.import_module(modname))\n                f.write(pydoc_output)\n            \n            modules.append((modname, os.path.basename(html_path)))\n            print(f\"Generated documentation for {modname}\")\n        except ImportError as e:\n            print(f\"Error importing {modname}: {e}\")\n    \n    # Create index.html\n    index_path = os.path.join(output_dir, 'index.html')\n    with open(index_path, 'w') as f:\n        f.write(\"&lt;html&gt;&lt;head&gt;&lt;title&gt;API Documentation&lt;/title&gt;&lt;/head&gt;&lt;body&gt;\\n\")\n        f.write(\"&lt;h1&gt;API Documentation&lt;/h1&gt;\\n&lt;ul&gt;\\n\")\n        \n        for modname, html_file in sorted(modules):\n            f.write(f'&lt;li&gt;&lt;a href=\"{html_file}\"&gt;{modname}&lt;/a&gt;&lt;/li&gt;\\n')\n        \n        f.write(\"&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt;\")\n    \n    print(f\"Index created at {index_path}\")\n\nif __name__ == \"__main__\":\n    # Change 'my_package' to your actual package name\n    generate_docs('my_package')\nThis script generates slightly more organized documentation than raw pydoc but still leverages built-in tools.\n\n\n4.1.4 Moving to MkDocs for Comprehensive Documentation\nWhen your project grows and needs more sophisticated documentation, MkDocs provides an excellent balance of simplicity and features. MkDocs generates a static site from Markdown files, making it easy to write and maintain documentation.\n\n4.1.4.1 Getting Started with MkDocs\nFirst, install MkDocs and a theme:\npip install mkdocs mkdocs-material\nInitialize a new documentation project:\nmkdocs new .\nThis creates a mkdocs.yml configuration file and a docs/ directory with an index.md file.\n\n\n4.1.4.2 Basic Configuration\nEdit mkdocs.yml:\nsite_name: My Project\ntheme:\n  name: material\n  palette:\n    primary: indigo\n    accent: indigo\nnav:\n  - Home: index.md\n  - User Guide:\n    - Installation: user-guide/installation.md\n    - Getting Started: user-guide/getting-started.md\n  - API Reference: api-reference.md\n  - Contributing: contributing.md\n\n\n4.1.4.3 Creating Documentation Content\nMkDocs uses Markdown files for content. Create docs/user-guide/installation.md:\n# Installation\n\n## Prerequisites\n\n- Python 3.8 or later\n- pip package manager\n\n## Installation Steps\n\n1. Install from PyPI:\n\n   ```bash\n   pip install my-package\n\nVerify installation:\npython -c \"import my_package; print(my_package.__version__)\"\n```\n\n\n\n4.1.4.4 Testing Documentation Locally\nPreview your documentation while writing:\nmkdocs serve\nThis starts a development server at http://127.0.0.1:8000 that automatically refreshes when you update files.\n\n\n4.1.4.5 Building and Deploying Documentation\nGenerate static HTML files:\nmkdocs build\nThis creates a site/ directory with the HTML documentation site.\nFor GitHub projects, you can publish to GitHub Pages:\nmkdocs gh-deploy\n\n\n\n4.1.5 Hosting Documentation with GitHub Pages\nGitHub Pages provides a simple, free hosting solution for your project documentation that integrates seamlessly with your GitHub repository.\n\n4.1.5.1 Setting Up GitHub Pages\nThere are two main approaches to hosting documentation on GitHub Pages:\n\nRepository site: Serves content from a dedicated branch (typically gh-pages)\nUser/Organization site: Serves content from a special repository named username.github.io\n\nFor most Python projects, the repository site approach works best:\n\nGo to your repository on GitHub\nNavigate to Settings → Pages\nUnder “Source”, select your branch (either main or gh-pages)\nChoose the folder that contains your documentation (/ or /docs)\nClick Save\n\nYour documentation will be published at https://username.github.io/repository-name/.\n\n\n4.1.5.2 Automating Documentation Deployment\nMkDocs has built-in support for GitHub Pages deployment:\n# Build and deploy documentation to GitHub Pages\nmkdocs gh-deploy\nThis command: 1. Builds your documentation into the site/ directory 2. Creates or updates the gh-pages branch 3. Pushes the built site to that branch 4. GitHub automatically serves the content\nFor a fully automated workflow, integrate this into your GitHub Actions CI pipeline:\nname: Deploy Documentation\n\non:\n  push:\n    branches:\n      - main\n    paths:\n      - 'docs/**'\n      - 'mkdocs.yml'\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install mkdocs mkdocs-material mkdocstrings[python]\n      - name: Deploy documentation\n        run: mkdocs gh-deploy --force\nThis workflow automatically deploys your documentation whenever you push changes to documentation files on the main branch.\n\n\n4.1.5.3 GitHub Pages with pydoc\nEven if you’re using the simpler pydoc approach, you can still host the generated HTML on GitHub Pages:\n\nCreate a docs/ folder in your repository\nGenerate HTML documentation with pydoc:\npython -m pydoc -w src/my_package/*.py\nmv *.html docs/\nAdd a simple docs/index.html that links to your module documentation\nConfigure GitHub Pages to serve from the docs/ folder of your main branch\n\n\n\n4.1.5.4 Custom Domains\nFor more established projects, you can use your own domain:\n\nPurchase a domain from a registrar\nAdd a CNAME file to your documentation with your domain name\nConfigure your DNS settings according to GitHub’s instructions\nEnable HTTPS in GitHub Pages settings\n\nBy hosting your documentation on GitHub Pages, you make it easily accessible to users and maintainable alongside your codebase. It’s a natural extension of the Git-based workflow we’ve established.\n\n\n4.1.5.5 Enhancing MkDocs\nMkDocs supports numerous plugins and extensions:\n\nCode highlighting: Built-in support for syntax highlighting\nAdmonitions: Create warning, note, and info boxes\nSearch: Built-in search functionality\nTable of contents: Automatic generation of section navigation\n\nExample of enhanced configuration:\nsite_name: My Project\ntheme:\n  name: material\n  features:\n    - navigation.instant\n    - navigation.tracking\n    - navigation.expand\n    - navigation.indexes\n    - content.code.annotate\nmarkdown_extensions:\n  - admonition\n  - pymdownx.highlight\n  - pymdownx.superfences\n  - toc:\n      permalink: true\nplugins:\n  - search\n  - mkdocstrings:\n      handlers:\n        python:\n          selection:\n            docstring_style: google\n\n\n\n4.1.6 Integrating API Documentation\nMkDocs alone is great for manual documentation, but you can also integrate auto-generated API documentation:\n\n4.1.6.1 Using mkdocstrings\nInstall mkdocstrings to include docstrings from your code:\npip install mkdocstrings[python]\nUpdate mkdocs.yml:\nplugins:\n  - search\n  - mkdocstrings:\n      handlers:\n        python:\n          selection:\n            docstring_style: google\nThen in your docs/api-reference.md:\n# API Reference\n\n## Module my_package.core\n\nThis module contains core functionality.\n\n::: my_package.core\n    options:\n      show_source: false\nThis automatically generates documentation from docstrings in your my_package.core module.\n\n\n\n4.1.7 Documentation Best Practices\nRegardless of which documentation tool you choose, follow these best practices:\n\nStart with a clear README: Include installation, quick start, and basic examples\nDocument as you code: Write documentation alongside code, not as an afterthought\nInclude examples: Show how to use functions and classes with realistic examples\nDocument edge cases and errors: Explain what happens in exceptional situations\nKeep documentation close to code: Use docstrings for API details\nMaintain a changelog: Track major changes between versions\nConsider different audiences: Write for both new users and experienced developers\n\n\n\n4.1.8 Choosing the Right Documentation Approach\n\n\n\n\n\n\n\nApproach\nWhen to Use\n\n\n\n\nDocstrings only\nFor very small, personal projects\n\n\npydoc\nFor simple projects with minimal documentation needs\n\n\nCustom pydoc script\nSmall to medium projects needing basic organization\n\n\nMkDocs\nMedium to large projects requiring structured, attractive documentation\n\n\nSphinx\nLarge, complex projects, especially with scientific or mathematical content\n\n\n\nFor most applications, the journey often progresses from simple docstrings to MkDocs as the project grows. By starting with good docstrings from the beginning, you make each subsequent step easier.\nIn the next section, we’ll explore how to automate your workflow with CI/CD using GitHub Actions."
  },
  {
    "objectID": "chapters/03-documentation.html#cicd-workflows-with-github-actions",
    "href": "chapters/03-documentation.html#cicd-workflows-with-github-actions",
    "title": "4  Documentation and Deployment",
    "section": "4.2 CI/CD Workflows with GitHub Actions",
    "text": "4.2 CI/CD Workflows with GitHub Actions\nContinuous Integration (CI) and Continuous Deployment (CD) automate the process of testing, building, and deploying your code, ensuring quality and consistency throughout the development lifecycle. GitHub Actions provides a powerful and flexible way to implement CI/CD workflows directly within your GitHub repository.\n\n4.2.1 Understanding CI/CD Basics\nBefore diving into implementation, let’s understand what each component achieves:\n\nContinuous Integration: Automatically testing code changes when pushed to the repository\nContinuous Deployment: Automatically deploying code to testing, staging, or production environments\n\nA robust CI/CD pipeline typically includes:\n\nRunning tests\nVerifying code quality (formatting, linting)\nStatic analysis (type checking, security scanning)\nBuilding documentation\nBuilding and publishing packages or applications\nDeploying to environments\n\n\n\n4.2.2 Setting Up GitHub Actions\nGitHub Actions workflows are defined using YAML files stored in the .github/workflows/ directory of your repository. Each workflow file defines a set of jobs and steps that execute in response to specified events.\nStart by creating the directory structure:\nmkdir -p .github/workflows\n\n\n4.2.3 Basic Python CI Workflow\nLet’s create a file named .github/workflows/ci.yml:\nname: Python CI\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.8\", \"3.9\", \"3.10\"]\n\n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n        cache: pip\n    \n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n        pip install -r requirements-dev.txt\n    \n    - name: Check formatting with Ruff\n      run: ruff format --check .\n    \n    - name: Lint with Ruff\n      run: ruff check .\n    \n    - name: Type check with mypy\n      run: mypy src/\n    \n    - name: Run security checks with Bandit\n      run: bandit -r src/ -x tests/\n    \n    - name: Test with pytest\n      run: pytest --cov=src/ --cov-report=xml\n    \n    - name: Upload coverage to Codecov\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n        fail_ci_if_error: true\nThis workflow:\n\nTriggers on pushes to main and on pull requests\nRuns on the latest Ubuntu environment\nTests against multiple Python versions\nSets up caching to speed up dependency installation\nRuns our full suite of quality checks and tests\nUploads coverage reports to Codecov (if you’ve set up this integration)\n\n\n\n4.2.4 Using Dependency Caching\nTo speed up your workflow, GitHub Actions provides caching capabilities:\n- name: Set up Python ${{ matrix.python-version }}\n  uses: actions/setup-python@v4\n  with:\n    python-version: ${{ matrix.python-version }}\n    cache: pip  # Enable pip caching\nFor more specific control over caching:\n- name: Cache pip packages\n  uses: actions/cache@v3\n  with:\n    path: ~/.cache/pip\n    key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}\n    restore-keys: |\n      ${{ runner.os }}-pip-\n\n\n4.2.5 Adapting for Different Dependency Tools\nIf you’re using uv instead of pip, adjust your workflow:\n- name: Install uv\n  run: curl -LsSf https://astral.sh/uv/install.sh | sh\n\n- name: Install dependencies with uv\n  run: |\n    uv pip sync requirements.txt requirements-dev.txt\n\n\n4.2.6 Building and Publishing Documentation\nAdd a job to build documentation with MkDocs:\ndocs:\n  runs-on: ubuntu-latest\n  steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: \"3.10\"\n    \n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install mkdocs mkdocs-material mkdocstrings[python]\n    \n    - name: Build documentation\n      run: mkdocs build --strict\n    \n    - name: Deploy to GitHub Pages\n      if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n      uses: peaceiris/actions-gh-pages@v3\n      with:\n        github_token: ${{ secrets.GITHUB_TOKEN }}\n        publish_dir: ./site\nThis job builds your documentation with MkDocs and deploys it to GitHub Pages when changes are pushed to the main branch.\n\n\n4.2.7 Building and Publishing Python Packages\nFor projects that produce packages, add a job for publication to PyPI:\npublish:\n  needs: [test, docs]  # Only run if test and docs jobs pass\n  runs-on: ubuntu-latest\n  # Only publish on tagged releases\n  if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags')\n  steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: \"3.10\"\n    \n    - name: Install build dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install build twine\n    \n    - name: Build package\n      run: python -m build\n    \n    - name: Check package with twine\n      run: twine check dist/*\n    \n    - name: Publish package\n      uses: pypa/gh-action-pypi-publish@release/v1\n      with:\n        user: __token__\n        password: ${{ secrets.PYPI_API_TOKEN }}\nThis job: 1. Only runs after tests and documentation have passed 2. Only triggers on tagged commits (releases) 3. Builds the package using the build package 4. Validates the package with twine 5. Publishes to PyPI using a secure token\nYou would need to add the PYPI_API_TOKEN to your repository secrets.\n\n\n4.2.8 Running Tests in Multiple Environments\nFor applications that need to support multiple operating systems or Python versions:\ntest:\n  runs-on: ${{ matrix.os }}\n  strategy:\n    matrix:\n      os: [ubuntu-latest, windows-latest, macos-latest]\n      python-version: [\"3.8\", \"3.9\", \"3.10\"]\n  \n  steps:\n    # ... Steps as before ...\nThis configuration runs your tests on three operating systems with three Python versions each, for a total of nine environments.\n\n\n4.2.9 Branch Protection and Required Checks\nTo ensure code quality, set up branch protection rules on GitHub:\n\nGo to your repository → Settings → Branches\nAdd a rule for your main branch\nEnable “Require status checks to pass before merging”\nSelect the checks from your CI workflow\n\nThis prevents merging pull requests until all tests pass, maintaining your code quality standards.\n\n\n4.2.10 Scheduled Workflows\nRun your tests on a schedule to catch issues with external dependencies:\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n  schedule:\n    - cron: '0 0 * * 0'  # Weekly on Sundays at midnight\n\n\n4.2.11 Notifications and Feedback\nConfigure notifications for workflow results:\n- name: Send notification\n  if: always()\n  uses: rtCamp/action-slack-notify@v2\n  env:\n    SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}\n    SLACK_TITLE: CI Result\n    SLACK_MESSAGE: ${{ job.status }}\n    SLACK_COLOR: ${{ job.status == 'success' && 'good' || 'danger' }}\nThis example sends notifications to Slack, but similar actions exist for other platforms.\n\n\n4.2.12 A Complete CI/CD Workflow Example\nHere’s a comprehensive workflow example bringing together many of the concepts we’ve covered:\nname: Python CI/CD Pipeline\n\non:\n  push:\n    branches: [ main ]\n    tags: [ 'v*' ]\n  pull_request:\n    branches: [ main ]\n  schedule:\n    - cron: '0 0 * * 0'  # Weekly on Sundays\n\njobs:\n  quality:\n    name: Code Quality\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n          cache: pip\n      \n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements-dev.txt\n      \n      - name: Check formatting\n        run: ruff format --check .\n      \n      - name: Lint with Ruff\n        run: ruff check .\n      \n      - name: Type check\n        run: mypy src/\n      \n      - name: Security scan\n        run: bandit -r src/ -x tests/\n      \n      - name: Check for dead code\n        run: vulture src/ --min-confidence 80\n\n  test:\n    name: Test\n    needs: quality\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest]\n        python-version: [\"3.8\", \"3.9\", \"3.10\"]\n        include:\n          - os: windows-latest\n            python-version: \"3.10\"\n          - os: macos-latest\n            python-version: \"3.10\"\n    \n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n          cache: pip\n      \n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt -r requirements-dev.txt\n      \n      - name: Test with pytest\n        run: pytest --cov=src/ --cov-report=xml\n      \n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./coverage.xml\n\n  docs:\n    name: Documentation\n    needs: quality\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      \n      - name: Install docs dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install mkdocs mkdocs-material mkdocstrings[python]\n      \n      - name: Build docs\n        run: mkdocs build --strict\n      \n      - name: Deploy docs\n        if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./site\n\n  publish:\n    name: Publish Package\n    needs: [test, docs]\n    runs-on: ubuntu-latest\n    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags')\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n      \n      - name: Install build dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install build twine\n      \n      - name: Build package\n        run: python -m build\n      \n      - name: Check package\n        run: twine check dist/*\n      \n      - name: Publish to PyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          user: __token__\n          password: ${{ secrets.PYPI_API_TOKEN }}\n      \n      - name: Create GitHub Release\n        uses: softprops/action-gh-release@v1\n        with:\n          files: dist/*\n          generate_release_notes: true\nThis comprehensive workflow: 1. Checks code quality (formatting, linting, type checking, security, dead code) 2. Runs tests on multiple Python versions and operating systems 3. Builds and deploys documentation 4. Publishes packages to PyPI on tagged releases 5. Creates GitHub releases with release notes\n\n\n4.2.13 CI/CD Best Practices\n\nKeep workflows modular: Split complex workflows into logical jobs\nFail fast: Run quick checks (like formatting) before longer ones (like testing)\nCache dependencies: Speed up workflows by caching pip packages\nBe selective: Only run necessary jobs based on changed files\nTest thoroughly: Include all environments your code supports\nSecure secrets: Use GitHub’s secret storage for tokens and keys\nMonitor performance: Watch workflow execution times and optimize slow steps\n\nWith these CI/CD practices in place, your development workflow becomes more reliable and automatic. Quality checks run on every change, documentation stays up to date, and releases happen smoothly and consistently.\nIn the final section, we’ll explore how to publish and distribute Python packages to make your work available to others."
  },
  {
    "objectID": "chapters/03-documentation.html#package-publishing-and-distribution",
    "href": "chapters/03-documentation.html#package-publishing-and-distribution",
    "title": "4  Documentation and Deployment",
    "section": "4.3 Package Publishing and Distribution",
    "text": "4.3 Package Publishing and Distribution\nWhen your Python project matures, you may want to share it with others through the Python Package Index (PyPI). Publishing your package makes it installable via pip, allowing others to easily use your work.\n\n4.3.1 Preparing Your Package for Distribution\nBefore publishing, your project needs the right structure. Let’s ensure everything is ready:\n\n4.3.1.1 1. Package Structure Review\nA distributable package should have this basic structure:\nmy_project/\n├── src/\n│   └── my_package/\n│       ├── __init__.py\n│       ├── module1.py\n│       └── module2.py\n├── tests/\n├── docs/\n├── pyproject.toml\n├── LICENSE\n└── README.md\n\n\n4.3.1.2 2. Package Configuration with pyproject.toml\nModern Python packaging uses pyproject.toml for configuration:\n[build-system]\nrequires = [\"setuptools&gt;=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\nversion = \"0.1.0\"\ndescription = \"A short description of my package\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.8\"\nlicense = {text = \"MIT\"}\nauthors = [\n    {name = \"Your Name\", email = \"your.email@example.com\"}\n]\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n]\ndependencies = [\n    \"requests&gt;=2.25.0\",\n    \"numpy&gt;=1.20.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=7.0.0\",\n    \"pytest-cov\",\n    \"ruff\",\n    \"mypy\",\n]\ndoc = [\n    \"mkdocs\",\n    \"mkdocs-material\",\n    \"mkdocstrings[python]\",\n]\n\n[project.urls]\n\"Homepage\" = \"https://github.com/yourusername/my-package\"\n\"Bug Tracker\" = \"https://github.com/yourusername/my-package/issues\"\n\n[project.scripts]\nmy-command = \"my_package.cli:main\"\n\n[tool.setuptools]\npackage-dir = {\"\" = \"src\"}\npackages = [\"my_package\"]\nThis configuration: - Defines basic metadata (name, version, description) - Lists dependencies (both required and optional) - Sets up entry points for command-line scripts - Specifies the package location (src layout)\n\n\n4.3.1.3 3. Include Essential Files\nEnsure you have these files:\n# Create a LICENSE file (example: MIT License)\ncat &gt; LICENSE &lt;&lt; EOF\nMIT License\n\nCopyright (c) $(date +%Y) Your Name\n\nPermission is hereby granted...\nEOF\n\n# Create a comprehensive README.md with:\n# - What the package does\n# - Installation instructions\n# - Basic usage examples\n# - Links to documentation\n# - Contributing guidelines\n\n\n\n4.3.2 Building Your Package\nWith configuration in place, you’re ready to build distribution packages:\n# Install build tools\npip install build\n\n# Build both wheel and source distribution\npython -m build\nThis creates two files in the dist/ directory: - A source distribution (.tar.gz) - A wheel file (.whl)\nAlways check your distributions before publishing:\n# Install twine\npip install twine\n\n# Check the package\ntwine check dist/*\n\n\n4.3.3 Publishing to Test PyPI\nBefore publishing to the real PyPI, test your package on TestPyPI:\n\nCreate a TestPyPI account at https://test.pypi.org/account/register/\nUpload your package:\n\ntwine upload --repository testpypi dist/*\n\nTest installation from TestPyPI:\n\npip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ my-package\n\n\n4.3.4 Publishing to PyPI\nWhen everything works correctly on TestPyPI:\n\nCreate a PyPI account at https://pypi.org/account/register/\nUpload your package:\n\ntwine upload dist/*\nYour package is now available to the world via pip install my-package!\n\n\n4.3.5 Automating Package Publishing\nTo automate publishing with GitHub Actions, add a workflow that: 1. Builds the package 2. Uploads to PyPI when you create a release tag\nname: Publish Python Package\n\non:\n  release:\n    types: [created]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.10'\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install build twine\n    - name: Build and publish\n      env:\n        TWINE_USERNAME: ${{ secrets.PYPI_USERNAME }}\n        TWINE_PASSWORD: ${{ secrets.PYPI_PASSWORD }}\n      run: |\n        python -m build\n        twine upload dist/*\nFor better security, use API tokens instead of your PyPI password: 1. Generate a token from your PyPI account settings 2. Add it as a GitHub repository secret 3. Use the token in your workflow:\n- name: Build and publish\n  env:\n    TWINE_USERNAME: __token__\n    TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}\n  run: |\n    python -m build\n    twine upload dist/*\n\n\n4.3.6 Versioning Best Practices\nFollow Semantic Versioning (MAJOR.MINOR.PATCH): - MAJOR: Incompatible API changes - MINOR: New functionality (backward-compatible) - PATCH: Bug fixes (backward-compatible)\nTrack versions in one place, usually in __init__.py:\n# src/my_package/__init__.py\n__version__ = \"0.1.0\"\nOr with a dynamic version from your git tags using setuptools-scm:\n[build-system]\nrequires = [\"setuptools&gt;=61.0\", \"wheel\", \"setuptools_scm[toml]&gt;=6.2\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools_scm]\n# Uses git tags for versioning\n\n\n4.3.7 Creating Releases\nA good release process includes:\n\nUpdate documentation:\n\nEnsure README is current\nUpdate changelog with notable changes\n\nCreate a new version:\n\nUpdate version number\nCreate a git tag:\ngit tag -a v0.1.0 -m \"Release version 0.1.0\"\ngit push origin v0.1.0\n\nMonitor the CI/CD pipeline:\n\nEnsure tests pass\nVerify package build succeeds\nConfirm successful publication\n\nAnnounce the release:\n\nCreate GitHub release notes\nPost in relevant community forums\nUpdate documentation site\n\n\n\n\n4.3.8 Package Maintenance\nOnce published, maintain your package responsibly:\n\nMonitor issues on GitHub or GitLab\nRespond to bug reports promptly\nReview and accept contributions from the community\nRegularly update dependencies to address security issues\nCreate new releases when significant improvements are ready\n\n\n\n4.3.9 Advanced Distribution Topics\nAs your package ecosystem grows, consider these advanced techniques:\n\n4.3.9.1 1. Binary Extensions\nFor performance-critical components, you might include compiled C extensions: - Use Cython to compile Python to C - Configure with the build-system section in pyproject.toml - Build platform-specific wheels\n\n\n4.3.9.2 2. Namespace Packages\nFor large projects split across multiple packages:\n# src/myorg/packageone/__init__.py\n# src/myorg/packagetwo/__init__.py\n\n# Makes 'myorg' a namespace package\n\n\n4.3.9.3 3. Conditional Dependencies\nFor platform-specific dependencies:\ndependencies = [\n    \"requests&gt;=2.25.0\",\n    \"numpy&gt;=1.20.0\",\n    \"pywin32&gt;=300; platform_system == 'Windows'\",\n]\n\n\n4.3.9.4 4. Data Files\nInclude non-Python files (data, templates, etc.):\n[tool.setuptools]\npackage-dir = {\"\" = \"src\"}\npackages = [\"my_package\"]\ninclude-package-data = true\nCreate a MANIFEST.in file:\ninclude src/my_package/data/*.json\ninclude src/my_package/templates/*.html\nBy following these practices, you’ll create a professional, well-maintained package that others can easily discover, install, and use. Publishing your work to PyPI is not just about sharing code—it’s about participating in the Python ecosystem and contributing back to the community.\n\n\n\n4.3.10 Modern vs. Traditional Python Packaging\nPython packaging has evolved significantly over the years:\n\n4.3.10.1 Traditional setup.py Approach\nHistorically, Python packages required a setup.py file:\n# setup.py\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"my-package\",\n    version=\"0.1.0\",\n    packages=find_packages(),\n    install_requires=[\n        \"requests&gt;=2.25.0\",\n        \"numpy&gt;=1.20.0\",\n    ],\n)\nThis approach is still common and has advantages for: - Compatibility with older tooling - Dynamic build processes that need Python code - Complex build requirements (e.g., C extensions, custom steps)\n\n\n4.3.10.2 Modern pyproject.toml Approach\nSince PEP 517/518, packages can use pyproject.toml exclusively:\n[build-system]\nrequires = [\"setuptools&gt;=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\nversion = \"0.1.0\"\ndependencies = [\n    \"requests&gt;=2.25.0\",\n    \"numpy&gt;=1.20.0\",\n]\nThis declarative approach is recommended for new projects because it: - Provides a standardized configuration format - Supports multiple build systems (not just setuptools) - Simplifies dependency specification - Avoids executing Python code during installation\n\n\n4.3.10.3 Which Approach Should You Use?\n\nFor new, straightforward packages: Use pyproject.toml only\nFor packages with complex build requirements: You may need both pyproject.toml and setup.py\nFor maintaining existing packages: Consider gradually migrating to pyproject.toml\n\nMany projects use a hybrid approach, with basic metadata in pyproject.toml and complex build logic in setup.py."
  },
  {
    "objectID": "chapters/04-case-study.html#project-overview",
    "href": "chapters/04-case-study.html#project-overview",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.1 Project Overview",
    "text": "5.1 Project Overview\nSimpleBot is an educational tool that makes it easy for students to interact with Large Language Models through simple Python functions. Key features include:\n\nSimple API for sending prompts to LLMs\nPre-defined personality bots (pirate, Shakespeare, emoji, etc.)\nError handling and user-friendly messages\nSupport for local LLM servers like Ollama\n\nThis project is ideal for our case study because: - It solves a real problem (making LLMs accessible in educational settings) - It’s small enough to understand quickly but complex enough to demonstrate real workflow practices - It includes both pure Python and compiled Cython components\nLet’s see how we can develop this project using our Python development pipeline."
  },
  {
    "objectID": "chapters/04-case-study.html#setting-the-foundation",
    "href": "chapters/04-case-study.html#setting-the-foundation",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.2 1. Setting the Foundation",
    "text": "5.2 1. Setting the Foundation\n\n5.2.1 Project Structure\nWe’ll set up the project using the recommended src layout:\nsimplebot/\n├── src/\n│   └── simplebot/\n│       ├── __init__.py\n│       ├── core.py\n│       └── personalities.py\n├── tests/\n│   ├── __init__.py\n│   ├── test_core.py\n│   └── test_personalities.py\n├── docs/\n│   ├── index.md\n│   └── examples.md\n├── .gitignore\n├── README.md\n├── requirements.in\n├── pyproject.toml\n└── LICENSE\n\n\n5.2.2 Setting Up Version Control\nFirst, we initialize a Git repository and create a .gitignore file:\n# Initialize Git repository\ngit init\n\n# Create a file named README.md with the following contents:d .gitignore with the following contents:\n# Virtual environments\n.venv/\nvenv/\nenv/\n\n# Python cache files\n__pycache__/\n*.py[cod]\n*$py.class\n.pytest_cache/\n\n# Distribution / packaging\ndist/\nbuild/\n*.egg-info/\n\n# Cython generated files\n*.c\n*.so\n\n# Local development settings\n.env\n.vscode/\n\n# Coverage reports\nhtmlcov/\n.coverage\nEOF\n\n# Initial commit\ngit add .gitignore\ngit commit -m \"Initial commit: Add .gitignore\"\n\n\n5.2.3 Creating Essential Files\nLet’s create the basic files:\n# Create the project structure\nmkdir -p src/simplebot tests docs\n\n# Create a file name\n# SimpleBot\n\n&gt; LLMs made simple for students and educators\n\nSimpleBot is a lightweight Python wrapper that simplifies interactions with Large Language Models (LLMs) for educational settings.\n\n## Installation\n\n\\`\\`\\`bash\npip install simplebot\n\\`\\`\\`\n\n## Quick Start\n\n\\`\\`\\`python\nfrom simplebot import get_response, pirate_bot\n\n# Basic usage\nresponse = get_response(\"Tell me about planets\")\nprint(response)\n\n# Use a personality bot\npirate_response = pirate_bot(\"Tell me about sailing ships\")\nprint(pirate_response)\n\\`\\`\\`\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\nEOF\n\n# Create a file named LICENSE with the following contents:\nMIT License\n\nCopyright (c) 2025 SimpleBot Authors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\nEOF\n\ngit add README.md LICENSE\ngit commit -m \"Add README and LICENSE\"\n\n\n5.2.4 Virtual Environment Setup\nWe’ll create a virtual environment and install basic development packages:\n# Create virtual environment\npython -m venv .venv\n\n# Activate the environment (Linux/macOS)\nsource .venv/bin/activate\n# On Windows: .venv\\Scripts\\activate\n\n# Initial package installation for development\npip install pytest ruff mypy build"
  },
  {
    "objectID": "chapters/04-case-study.html#building-the-core-functionality",
    "href": "chapters/04-case-study.html#building-the-core-functionality",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.3 2. Building the Core Functionality",
    "text": "5.3 2. Building the Core Functionality\nLet’s start with the core module implementation:\n# Create the package structure\nmkdir -p src/simplebot\n# Create the package __init__.py\n# Create a file named src/simplebot/__init__.py with the following contents:\n\"\"\"SimpleBot - LLMs made simple for students and educators.\"\"\"\n\nfrom .core import get_response\nfrom .personalities import (\n    pirate_bot,\n    shakespeare_bot,\n    emoji_bot,\n    teacher_bot,\n    coder_bot,\n)\n\n__version__ = \"0.1.0\"\n\n__all__ = [\n    \"get_response\",\n    \"pirate_bot\",\n    \"shakespeare_bot\",\n    \"teacher_bot\",\n    \"emoji_bot\",\n    \"coder_bot\",\n]\n\n# Create the core module\n# Create a file named src/simplebot/core.py with the following contents:\n\"\"\"Core functionality for SimpleBot.\"\"\"\n\nimport requests\nimport random\nimport time\nfrom typing import Optional, Dict, Any\n\n# Cache for the last used model to avoid redundant loading messages\n_last_model: Optional[str] = None\n\ndef get_response(\n    prompt: str,\n    model: str = \"llama3\",\n    system: str = \"You are a helpful assistant.\",\n    stream: bool = False,\n    api_url: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Send a prompt to the LLM API and retrieve the model's response.\n\n    Args:\n        prompt: The text prompt to send to the language model\n        model: The name of the model to use\n        system: System instructions that control the model's behavior\n        stream: Whether to stream the response\n        api_url: Custom API URL (defaults to local Ollama server)\n\n    Returns:\n        The model's response text, or an error message if the request fails\n    \"\"\"\n    global _last_model\n    \n    # Default to local Ollama if no API URL is provided\n    if api_url is None:\n        api_url = \"http://localhost:11434/api/generate\"\n\n    # Handle model switching with friendly messages\n    if model != _last_model:\n        warmup_messages = [\n            f\"🧠 Loading model '{model}' into RAM... give me a sec...\",\n            f\"💾 Spinning up the AI core for '{model}'...\",\n            f\"⏳ Summoning the knowledge spirits... '{model}' booting...\",\n            f\"🤖 Thinking really hard with '{model}'...\",\n            f\"⚙️ Switching to model: {model} ... (may take a few seconds)\",\n        ]\n        print(random.choice(warmup_messages))\n\n        # Short pause to simulate/allow for model loading\n        time.sleep(1.5)\n        _last_model = model\n\n    # Validate input\n    if not prompt.strip():\n        return \"⚠️ Empty prompt.\"\n\n    # Prepare the request payload\n    payload: Dict[str, Any] = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"system\": system,\n        \"stream\": stream\n    }\n\n    try:\n        # Send request to the LLM API\n        response = requests.post(\n            api_url,\n            json=payload,\n            timeout=10\n        )\n        response.raise_for_status()\n        data = response.json()\n        return data.get(\"response\", \"⚠️ No response from model.\")\n    except requests.RequestException as e:\n        return f\"❌ Connection Error: {str(e)}\"\n    except Exception as e:\n        return f\"❌ Error: {str(e)}\"\nEOF\n\n# Create the personalities module\n# Create a file named src/simplebot/personalities.py with the following contents:\n\"\"\"Pre-defined personality bots for SimpleBot.\"\"\"\n\nfrom .core import get_response\nfrom typing import Optional\n\ndef pirate_bot(prompt: str, model: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Generate a response in the style of a 1700s pirate with nautical slang.\n\n    Args:\n        prompt: The user's input text/question\n        model: Optional model override\n\n    Returns:\n        A response written in pirate vernacular\n    \"\"\"\n    return get_response(\n        prompt,\n        system=\"You are a witty pirate from the 1700s. \"\n               \"Use nautical slang, say 'arr' occasionally, \"\n               \"and reference sailing, treasure, and the sea.\",\n        model=model or \"llama3\"\n    )\n\ndef shakespeare_bot(prompt: str, model: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Generate a response in the style of William Shakespeare.\n\n    Args:\n        prompt: The user's input text/question\n        model: Optional model override\n\n    Returns:\n        A response written in Shakespearean style\n    \"\"\"\n    return get_response(\n        prompt,\n        system=\"You respond in the style of William Shakespeare, \"\n               \"using Early Modern English vocabulary and phrasing.\",\n        model=model or \"llama3\"\n    )\n\ndef emoji_bot(prompt: str, model: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Generate a response primarily using emojis with minimal text.\n\n    Args:\n        prompt: The user's input text/question\n        model: Optional model override\n\n    Returns:\n        A response composed primarily of emojis\n    \"\"\"\n    return get_response(\n        prompt,\n        system=\"You respond using mostly emojis, mixing minimal words \"\n               \"and symbols to convey meaning. You love using expressive \"\n               \"emoji strings.\",\n        model=model or \"llama3\"\n    )\n\ndef teacher_bot(prompt: str, model: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Generate a response in the style of a patient, helpful educator.\n\n    Args:\n        prompt: The user's input text/question\n        model: Optional model override\n\n    Returns:\n        A response with an educational approach\n    \"\"\"\n    return get_response(\n        prompt,\n        system=\"You are a patient, encouraging teacher who explains \"\n               \"concepts clearly at an appropriate level. Break down \"\n               \"complex ideas into simpler components and use analogies \"\n               \"when helpful.\",\n        model=model or \"llama3\"\n    )\n\ndef coder_bot(prompt: str, model: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Generate a response from a coding assistant optimized for programming help.\n\n    Args:\n        prompt: The user's input programming question or request\n        model: Optional model override (defaults to a coding-specific model)\n\n    Returns:\n        A technical response focused on code-related assistance\n    \"\"\"\n    return get_response(\n        prompt,\n        system=\"You are a skilled coding assistant who explains and writes \"\n               \"code clearly and concisely. Prioritize best practices, \"\n               \"readability, and proper error handling.\",\n        model=model or \"codellama\"\n    )\nEOF\n\ngit add src/\ngit commit -m \"Add core SimpleBot functionality\""
  },
  {
    "objectID": "chapters/04-case-study.html#package-configuration",
    "href": "chapters/04-case-study.html#package-configuration",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.4 3. Package Configuration",
    "text": "5.4 3. Package Configuration\nLet’s set up the package configuration in pyproject.toml:\n# Create pyproject.toml directory\n\nNote on Modern Packaging: This case study uses the newer pyproject.toml-only approach for simplicity and to follow current best practices. Many existing Python projects still use setup.py, either alongside pyproject.toml or as their primary configuration. The setup.py approach remains valuable for packages with complex build requirements, custom build steps, or when supporting older tools and Python versions. For SimpleBot, our straightforward package requirements allow us to use the cleaner, declarative pyproject.toml approach."
  },
  {
    "objectID": "chapters/04-case-study.html#create-a-file-named-pyproject.toml-with-the-following-contents",
    "href": "chapters/04-case-study.html#create-a-file-named-pyproject.toml-with-the-following-contents",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.5 Create a file named pyproject.toml with the following contents:",
    "text": "5.5 Create a file named pyproject.toml with the following contents:\nLet’s set up the package configuration in pyproject.toml:\n# Create a file named pyproject.toml with the following contents:\n[build-system]\nrequires = [\"setuptools&gt;=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"simplebot\"\nversion = \"0.1.0\"\ndescription = \"LLMs made simple for students and educators\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.7\"\nlicense = {text = \"MIT\"}\nauthors = [\n    {name = \"SimpleBot Team\", email = \"example@example.com\"}\n]\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n    \"Intended Audience :: Education\",\n    \"Topic :: Education :: Computer Aided Instruction (CAI)\",\n]\ndependencies = [\n    \"requests&gt;=2.25.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=7.0.0\",\n    \"pytest-cov\",\n    \"ruff\",\n    \"mypy\",\n]\n\n[project.urls]\n\"Homepage\" = \"https://github.com/simplebot-team/simplebot\"\n\"Bug Tracker\" = \"https://github.com/simplebot-team/simplebot/issues\"\n\n# Tool configurations\n[tool.ruff]\nselect = [\"E\", \"F\", \"I\"]\nline-length = 88\n\n[tool.ruff.per-file-ignores]\n\"__init__.py\" = [\"F401\"]\n\n[tool.mypy]\npython_version = \"3.7\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\ndisallow_incomplete_defs = true\n\n[[tool.mypy.overrides]]\nmodule = \"tests.*\"\ndisallow_untyped_defs = false\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\nEOF\n\n# Create requirements.in file\n# Create a file named requirements.in with the following contents:\n# Direct dependencies\nrequests&gt;=2.25.0\nEOF\n\n# Create requirements-dev.in\n# Create a file named requirements-dev.in with the following contents:\n# Development dependencies\npytest&gt;=7.0.0\npytest-cov\nruff\nmypy\nbuild\ntwine\nEOF\n\ngit add pyproject.toml requirements*.in\ngit commit -m \"Add package configuration and dependency files\""
  },
  {
    "objectID": "chapters/04-case-study.html#writing-tests",
    "href": "chapters/04-case-study.html#writing-tests",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.6 4. Writing Tests",
    "text": "5.6 4. Writing Tests\nLet’s create some tests for our SimpleBot functionality:\n# Create test files\n# Create a file named tests/__init__.py with the following contents:\n\"\"\"SimpleBot test package.\"\"\"\nEOF\n\n# Create a file named tests/test_core.py with the following contents:\n\"\"\"Tests for the SimpleBot core module.\"\"\"\n\nimport pytest\nfrom unittest.mock import patch, MagicMock\nfrom simplebot.core import get_response\n\n@patch(\"simplebot.core.requests.post\")\ndef test_successful_response(mock_post):\n    \"\"\"Test that a successful API response is handled correctly.\"\"\"\n    # Setup mock\n    mock_response = MagicMock()\n    mock_response.json.return_value = {\"response\": \"Test response\"}\n    mock_post.return_value = mock_response\n    \n    # Call function\n    result = get_response(\"Test prompt\")\n    \n    # Assertions\n    assert result == \"Test response\"\n    mock_post.assert_called_once()\n\n@patch(\"simplebot.core.requests.post\")\ndef test_empty_prompt(mock_post):\n    \"\"\"Test that empty prompts are handled correctly.\"\"\"\n    result = get_response(\"\")\n    assert \"Empty prompt\" in result\n    mock_post.assert_not_called()\n\n@patch(\"simplebot.core.requests.post\")\ndef test_api_error(mock_post):\n    \"\"\"Test that API errors are handled gracefully.\"\"\"\n    # Setup mock to raise an exception\n    mock_post.side_effect = Exception(\"Test error\")\n    \n    # Call function\n    result = get_response(\"Test prompt\")\n    \n    # Assertions\n    assert \"Error\" in result\n    assert \"Test error\" in result\nEOF\n\n# Create a file named tests/test_personalities.py with the following contents:\n\"\"\"Tests for the SimpleBot personalities module.\"\"\"\n\nimport pytest\nfrom unittest.mock import patch\nfrom simplebot import (\n    pirate_bot,\n    shakespeare_bot,\n    emoji_bot,\n    teacher_bot,\n    coder_bot,\n)\n\n@patch(\"simplebot.personalities.get_response\")\ndef test_pirate_bot(mock_get_response):\n    \"\"\"Test that pirate_bot calls get_response with correct parameters.\"\"\"\n    # Setup\n    mock_get_response.return_value = \"Arr, test response!\"\n    \n    # Call function\n    result = pirate_bot(\"Test prompt\")\n    \n    # Assertions\n    assert result == \"Arr, test response!\"\n    mock_get_response.assert_called_once()\n    # Check that system prompt contains pirate references\n    system_arg = mock_get_response.call_args[1][\"system\"]\n    assert \"pirate\" in system_arg.lower()\n\n@patch(\"simplebot.personalities.get_response\")\ndef test_custom_model(mock_get_response):\n    \"\"\"Test that personality bots accept custom model parameter.\"\"\"\n    # Setup\n    mock_get_response.return_value = \"Custom model response\"\n    \n    # Call functions with custom model\n    shakespeare_bot(\"Test\", model=\"custom-model\")\n    \n    # Assertions\n    assert mock_get_response.call_args[1][\"model\"] == \"custom-model\"\nEOF\n\ngit add tests/\ngit commit -m \"Add unit tests for SimpleBot\""
  },
  {
    "objectID": "chapters/04-case-study.html#applying-code-quality-tools",
    "href": "chapters/04-case-study.html#applying-code-quality-tools",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.7 5. Applying Code Quality Tools",
    "text": "5.7 5. Applying Code Quality Tools\nLet’s run our code quality tools and fix any issues:\n# Install development dependencies\npip install -r requirements-dev.in\n\n# Run Ruff for formatting and linting\nruff format .\nruff check .\n\n# Run mypy for type checking\nmypy src/\n\n# Fix any issues identified by the tools\ngit add .\ngit commit -m \"Apply code formatting and fix linting issues\""
  },
  {
    "objectID": "chapters/04-case-study.html#documentation",
    "href": "chapters/04-case-study.html#documentation",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.8 6. Documentation",
    "text": "5.8 6. Documentation\nLet’s create basic documentation:\n# Create docs directory\nmkdir -p docs\n\n# Create main documentation file\n# Create a file named docs/index.md with the following contents:\n# SimpleBot Documentation\n\n&gt; LLMs made simple for students and educators\n\nSimpleBot is a lightweight Python wrapper that simplifies interactions with Large Language Models (LLMs) for educational settings. It abstracts away the complexity of API calls, model management, and error handling, allowing students to focus on learning programming concepts through engaging AI interactions.\n\n## Installation\n\n\\`\\`\\`bash\npip install simplebot\n\\`\\`\\`\n\n## Basic Usage\n\n\\`\\`\\`python\nfrom simplebot import get_response\n\n# Basic usage with default model\nresponse = get_response(\"Tell me about planets\")\nprint(response)\n\\`\\`\\`\n\n## Personality Bots\n\nSimpleBot comes with several pre-defined personality bots:\n\n\\`\\`\\`python\nfrom simplebot import pirate_bot, shakespeare_bot, emoji_bot, teacher_bot, coder_bot\n\n# Get a response in pirate speak\npirate_response = pirate_bot(\"Tell me about sailing ships\")\nprint(pirate_response)\n\n# Get a response in Shakespearean style\nshakespeare_response = shakespeare_bot(\"What is love?\")\nprint(shakespeare_response)\n\n# Get a response with emojis\nemoji_response = emoji_bot(\"Explain happiness\")\nprint(emoji_response)\n\n# Get an educational response\nteacher_response = teacher_bot(\"How do photosynthesis work?\")\nprint(teacher_response)\n\n# Get coding help\ncode_response = coder_bot(\"Write a Python function to check if a string is a palindrome\")\nprint(code_response)\n\\`\\`\\`\n\n## API Reference\n\n### get_response()\n\n\\`\\`\\`python\ndef get_response(\n    prompt: str,\n    model: str = \"llama3\",\n    system: str = \"You are a helpful assistant.\",\n    stream: bool = False,\n    api_url: Optional[str] = None,\n) -&gt; str:\n\\`\\`\\`\n\nThe core function for sending prompts to an LLM and getting responses.\n\n#### Parameters:\n\n- `prompt`: The text prompt to send to the language model\n- `model`: The name of the model to use (default: \"llama3\")\n- `system`: System instructions that control the model's behavior\n- `stream`: Whether to stream the response (default: False)\n- `api_url`: Custom API URL (defaults to local Ollama server)\n\n#### Returns:\n\n- A string containing the model's response or an error message\nEOF\n\n# Create examples file\n# Create a file named docs/examples.md with the following contents:\n# SimpleBot Examples\n\nHere are some examples of using SimpleBot in educational settings.\n\n## Creating Custom Bot Personalities\n\nYou can create custom bot personalities:\n\n\\`\\`\\`python\nfrom simplebot import get_response\n\ndef scientist_bot(prompt):\n    \"\"\"A bot that responds like a scientific researcher.\"\"\"\n    return get_response(\n        prompt,\n        system=\"You are a scientific researcher. Provide evidence-based \"\n               \"responses with references to studies when possible. \"\n               \"Be precise and methodical in your explanations.\"\n    )\n\nresult = scientist_bot(\"What happens during photosynthesis?\")\nprint(result)\n\\`\\`\\`\n\n## Building a Simple Quiz System\n\n\\`\\`\\`python\nfrom simplebot import teacher_bot\n\nquiz_questions = [\n    \"What is the capital of France?\",\n    \"Who wrote Romeo and Juliet?\",\n    \"What is the chemical symbol for water?\"\n]\n\ndef generate_quiz():\n    print(\"=== Quiz Time! ===\")\n    for i, question in enumerate(quiz_questions, 1):\n        print(f\"Question {i}: {question}\")\n        user_answer = input(\"Your answer: \")\n        \n        # Generate feedback on the answer\n        feedback = teacher_bot(\n            f\"Question: {question}\\nStudent answer: {user_answer}\\n\"\n            \"Provide brief, encouraging feedback on whether this answer is \"\n            \"correct. If incorrect, provide the correct answer.\"\n        )\n        print(f\"Feedback: {feedback}\\n\")\n\n# Run the quiz\ngenerate_quiz()\n\\`\\`\\`\n\n## Simulating a Conversation Between Bots\n\n\\`\\`\\`python\nfrom simplebot import pirate_bot, shakespeare_bot\n\ndef bot_conversation(topic, turns=3):\n    \"\"\"Simulate a conversation between two bots on a given topic.\"\"\"\n    print(f\"=== A conversation about {topic} ===\")\n    \n    # Start with the pirate\n    current_message = f\"Tell me about {topic}\"\n    current_bot = \"pirate\"\n    \n    for i in range(turns):\n        if current_bot == \"pirate\":\n            response = pirate_bot(current_message)\n            print(f\"🏴‍☠️ Pirate: {response}\")\n            current_message = f\"Respond to this: {response}\"\n            current_bot = \"shakespeare\"\n        else:\n            response = shakespeare_bot(current_message)\n            print(f\"🎭 Shakespeare: {response}\")\n            current_message = f\"Respond to this: {response}\"\n            current_bot = \"pirate\"\n        print()\n\n# Run a conversation about the ocean\nbot_conversation(\"the ocean\", turns=4)\n\\`\\`\\`\nEOF\n\ngit add docs/\ngit commit -m \"Add documentation\""
  },
  {
    "objectID": "chapters/04-case-study.html#setup-cicd-with-github-actions",
    "href": "chapters/04-case-study.html#setup-cicd-with-github-actions",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.9 7. Setup CI/CD with GitHub Actions",
    "text": "5.9 7. Setup CI/CD with GitHub Actions\nNow let’s set up continuous integration:\n# Create GitHub Actions workflow directory\nmkdir -p .github/workflows\n\n# Create CI workflow file\n# Create a file named .github/workflows/ci.yml with the following contents:\nname: Python CI\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.7\", \"3.8\", \"3.9\", \"3.10\"]\n\n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python \\${{ matrix.python-version }}\n      uses: actions/setup-python@v4\n      with:\n        python-version: \\${{ matrix.python-version }}\n        cache: pip\n    \n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        python -m pip install -e \".[dev]\"\n    \n    - name: Check formatting with Ruff\n      run: ruff format --check .\n    \n    - name: Lint with Ruff\n      run: ruff check .\n    \n    - name: Type check with mypy\n      run: mypy src/\n    \n    - name: Test with pytest\n      run: pytest --cov=src/ tests/\n    \n    - name: Build package\n      run: python -m build\nEOF\n\n# Create release workflow\n# Create a file named .github/workflows/release.yml with the following contents:\nname: Publish to PyPI\n\non:\n  release:\n    types: [created]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: \"3.10\"\n    \n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install build twine\n    \n    - name: Build and publish\n      env:\n        TWINE_USERNAME: \\${{ secrets.PYPI_USERNAME }}\n        TWINE_PASSWORD: \\${{ secrets.PYPI_PASSWORD }}\n      run: |\n        python -m build\n        twine check dist/*\n        twine upload dist/*\nEOF\n\ngit add .github/\ngit commit -m \"Add CI/CD workflows\""
  },
  {
    "objectID": "chapters/04-case-study.html#finalizing-for-distribution",
    "href": "chapters/04-case-study.html#finalizing-for-distribution",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.10 8. Finalizing for Distribution",
    "text": "5.10 8. Finalizing for Distribution\nLet’s prepare for distribution:\n# Install the package in development mode\npip install -e .\n\n# Run the tests\npytest\n\n# Build the package\npython -m build\n\n# Verify the package\ntwine check dist/*"
  },
  {
    "objectID": "chapters/04-case-study.html#project-summary",
    "href": "chapters/04-case-study.html#project-summary",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.11 9. Project Summary",
    "text": "5.11 9. Project Summary\nBy following the Python Development Workflow, we’ve transformed the SimpleBot concept into a well-structured, tested, and documented Python package that’s ready for distribution. Let’s review what we’ve accomplished:\n\nProject Foundation:\n\nCreated a clear, organized directory structure\nSet up version control with Git\nAdded essential files (README, LICENSE)\n\nDevelopment Environment:\n\nCreated a virtual environment\nManaged dependencies cleanly\n\nCode Quality:\n\nApplied type hints throughout the codebase\nUsed Ruff for formatting and linting\nUsed mypy for static type checking\n\nTesting:\n\nCreated comprehensive unit tests with pytest\nUsed mocking to test external API interactions\n\nDocumentation:\n\nAdded clear docstrings\nCreated usage documentation with examples\n\nPackaging & Distribution:\n\nConfigured the package with pyproject.toml\nSet up CI/CD with GitHub Actions"
  },
  {
    "objectID": "chapters/04-case-study.html#next-steps",
    "href": "chapters/04-case-study.html#next-steps",
    "title": "5  Case Study: Building SimpleBot - A Python Development Workflow Example",
    "section": "5.12 10. Next Steps",
    "text": "5.12 10. Next Steps\nIf we were to continue developing SimpleBot, potential next steps might include:\n\nEnhanced Features:\n\nAdd more personality bots\nSupport for conversation memory/context\nConfiguration file support\n\nAdvanced Documentation:\n\nSet up MkDocs for a full documentation site\nAdd tutorials for classroom usage\n\nPerformance Improvements:\n\nAdd caching for responses\nImplement Cython optimization for performance-critical sections\n\nSecurity Enhancements:\n\nAdd API key management\nImplement content filtering for educational settings\n\n\nThis case study demonstrates how following a structured Python development workflow leads to a high-quality, maintainable, and distributable package—even for relatively small projects."
  },
  {
    "objectID": "chapters/05-advanced-techniques.html",
    "href": "chapters/05-advanced-techniques.html",
    "title": "6  Advanced Development Techniques",
    "section": "",
    "text": "This section could focus on taking Python development to the next level beyond the fundamentals covered in earlier parts:\n\nContainerization with Docker\n\nCreating development containers\nMulti-stage builds for Python applications\nDocker Compose for development environments\nProduction containers and security considerations\n\nPerformance Optimization\n\nProfiling tools and techniques\nMemory usage analysis and optimization\nCPU bottleneck identification\nJust-in-time compilation with Numba\nCython for performance-critical code\n\nCross-Platform Development\n\nManaging path differences between operating systems\nPlatform-specific code and dependencies\nBuilding standalone applications with PyInstaller\nPlatform-specific packaging considerations\n\nDatabase Integration Best Practices\n\nORM patterns with SQLAlchemy\nConnection management and pooling\nMigration strategies\nTesting with database fixtures\n\nAPI Development and Integration\n\nRESTful API design in Python\nAsyncIO for high-performance services\nOpenAPI/Swagger integration\nAPI testing strategies"
  },
  {
    "objectID": "chapters/06-project-management.html",
    "href": "chapters/06-project-management.html",
    "title": "7  Project Management for Python Teams",
    "section": "",
    "text": "This section would focus on the human and organizational aspects of Python development:\n\nTeam Collaboration Workflows\n\nCode review best practices\nPull request templates and processes\nDocumentation standards for teams\nKnowledge sharing techniques\n\nMonitoring and Observability\n\nStructured logging best practices\nError tracking and alerting\nPerformance monitoring\nCentralized logging systems\n\nManaging Technical Debt\n\nIdentifying code smells in Python\nRefactoring strategies and patterns\nMeasuring and tracking code quality\nPlanning and prioritizing technical improvements\n\nRelease Management\n\nVersioning strategies\nManaging deprecations\nRelease checklists and processes\nHandling backward compatibility\n\nCommunity and Open Source Engagement\n\nBuilding and managing open source Python projects\nDocumentation for community contributors\nIssue and PR management\nBuilding community around Python projects"
  },
  {
    "objectID": "chapters/07-conclusion.html#the-power-of-a-complete-pipeline",
    "href": "chapters/07-conclusion.html#the-power-of-a-complete-pipeline",
    "title": "8  Conclusion: Embracing Efficient Python Development",
    "section": "8.1 The Power of a Complete Pipeline",
    "text": "8.1 The Power of a Complete Pipeline\nEach component of our development workflow serves a specific purpose:\n\nProject structure provides organization and clarity\nVersion control enables collaboration and change tracking\nVirtual environments isolate dependencies\nDependency management ensures reproducible environments\nCode formatting and linting maintain consistent, error-free code\nTesting verifies functionality\nType checking catches type errors early\nSecurity scanning prevents vulnerabilities\nDead code detection keeps projects lean\nDocumentation makes code accessible to others\nCI/CD automates quality checks and deployment\nPackage publishing shares your work with the world\n\nTogether, these practices create a development experience that is both efficient and enjoyable. You spend less time on repetitive tasks and more time solving the real problems your code addresses."
  },
  {
    "objectID": "chapters/07-conclusion.html#adapting-to-your-needs",
    "href": "chapters/07-conclusion.html#adapting-to-your-needs",
    "title": "8  Conclusion: Embracing Efficient Python Development",
    "section": "8.2 Adapting to Your Needs",
    "text": "8.2 Adapting to Your Needs\nWhile we’ve presented a full-featured workflow, remember that you don’t need to implement everything at once:\n\nStart small: Begin with basic structure, version control, and virtual environments\nAdd incrementally: Introduce code quality tools, testing, and type checking as your project grows\nAutomate progressively: Add CI/CD as manual processes become burdensome\nEvolve documentation: Scale from simple README to comprehensive documentation sites as needed\n\nThis tiered approach lets you adopt best practices at a pace that matches your project’s complexity and your team’s capacity."
  },
  {
    "objectID": "chapters/07-conclusion.html#beyond-tools-engineering-culture",
    "href": "chapters/07-conclusion.html#beyond-tools-engineering-culture",
    "title": "8  Conclusion: Embracing Efficient Python Development",
    "section": "8.3 Beyond Tools: Engineering Culture",
    "text": "8.3 Beyond Tools: Engineering Culture\nThe most important outcome isn’t just using specific tools—it’s developing habits and values that lead to better software:\n\nThink defensively: Use tools that catch mistakes early\nValue maintainability: Write code for humans, not just computers\nEmbrace automation: Let computers handle repetitive tasks\nPractice continuous improvement: Regularly refine your workflow\nShare knowledge: Document not just what code does, but why"
  },
  {
    "objectID": "chapters/07-conclusion.html#when-to-consider-more-advanced-tools",
    "href": "chapters/07-conclusion.html#when-to-consider-more-advanced-tools",
    "title": "8  Conclusion: Embracing Efficient Python Development",
    "section": "8.4 When to Consider More Advanced Tools",
    "text": "8.4 When to Consider More Advanced Tools\nAs your projects grow more complex, you might explore more sophisticated tools:\n\nContainerization with Docker for consistent environments\nOrchestration with Kubernetes for complex deployments\nMonorepo tools like Pants or Bazel for large codebases\nFeature flagging for controlled feature rollouts\nAdvanced monitoring for production insights\n\nHowever, the core practices we’ve covered will remain valuable regardless of the scale you reach."
  },
  {
    "objectID": "chapters/07-conclusion.html#staying-updated",
    "href": "chapters/07-conclusion.html#staying-updated",
    "title": "8  Conclusion: Embracing Efficient Python Development",
    "section": "8.5 Staying Updated",
    "text": "8.5 Staying Updated\nPython’s ecosystem continues to evolve. Stay current by:\n\nFollowing Python Enhancement Proposals (PEPs)\nParticipating in community discussions\nTesting new tools in small projects before adoption\nReading release notes for your dependencies\nAttending conferences or meetups (virtual or in-person)"
  },
  {
    "objectID": "chapters/07-conclusion.html#final-thoughts",
    "href": "chapters/07-conclusion.html#final-thoughts",
    "title": "8  Conclusion: Embracing Efficient Python Development",
    "section": "8.6 Final Thoughts",
    "text": "8.6 Final Thoughts\nEffective Python development isn’t about using every available tool—it’s about creating a workflow that enhances your productivity and code quality while minimizing friction. By implementing the practices in this guide, you’ve built a foundation that will serve you well across projects of all sizes.\nRemember that you don’t need to implement all these practices manually for each new project. The companion cookiecutter template (available at [URL]) encapsulates the workflow described in this book, allowing you to focus on writing code rather than setting up infrastructure.\nRemember that perfect is the enemy of good. Start with the basics, improve incrementally, and focus on delivering value through your code. The best development pipeline is one that you’ll actually use consistently.\nWe hope this guide helps you on your journey to more effective, enjoyable Python development. Happy coding!"
  },
  {
    "objectID": "acknowledgments.html#author",
    "href": "acknowledgments.html#author",
    "title": "Acknowledgments",
    "section": "Author",
    "text": "Author\nMichael Borck (michael@borck.me) - Lead author and creator. Michael developed the core concepts, structured the book, and wrote the original content for “From Zero to Production: A Practical Python Development Pipeline.”"
  },
  {
    "objectID": "acknowledgments.html#ai-assistance",
    "href": "acknowledgments.html#ai-assistance",
    "title": "Acknowledgments",
    "section": "AI Assistance",
    "text": "AI Assistance\nThis book was developed with assistance from several AI tools:\n\nClaude by Anthropic - Provided editorial suggestions, helped refine concepts, and assisted with book structure and content development.\nMidjourney AI - Generated the cover artwork based on prompts describing the book’s themes of Python development pipelines."
  },
  {
    "objectID": "acknowledgments.html#technical-production",
    "href": "acknowledgments.html#technical-production",
    "title": "Acknowledgments",
    "section": "Technical Production",
    "text": "Technical Production\n\nQuarto - Used for document formatting and book generation\nGitHub - Used for version control and collaboration\nGitHub Pages - Hosts the online version of the book"
  },
  {
    "objectID": "acknowledgments.html#special-thanks",
    "href": "acknowledgments.html#special-thanks",
    "title": "Acknowledgments",
    "section": "Special Thanks",
    "text": "Special Thanks\nSpecial thanks to the Python development community whose tools, frameworks, and best practices form the foundation of this book. The vibrant ecosystem of Python developers continually pushing the boundaries of what’s possible with the language has been an inspiration.\nAlso thanks to the educators and mentors who emphasize practical, sustainable development practices over quick-but-fragile solutions.\n\nNote: While AI tools were used in the production of this book, all content reflects the author’s intentions and has been reviewed by humans. The Python development practices presented aim to balance simplicity with robustness - embracing the book’s theme of “Simple but not Simplistic.”"
  },
  {
    "objectID": "appendices/a-checklist.html#project-progression-path",
    "href": "appendices/a-checklist.html#project-progression-path",
    "title": "Appendix A — Appendix A: Python Development Workflow Checklist",
    "section": "A.1 Project Progression Path",
    "text": "A.1 Project Progression Path\nFor projects that start simple but grow in complexity, follow this progression:\n\nStart with the essentials:\n\nProject structure and version control\nVirtual environment\nBasic testing\nClear README\n\nAdd code quality tools incrementally:\n\nFirst add Ruff for formatting and basic linting\nThen add mypy for critical modules\nFinally add security scanning\n\nEnhance testing as complexity increases:\n\nAdd coverage reporting\nImplement mocking for external dependencies\nAdd integration tests for component interactions\n\nImprove documentation with growth:\n\nStart with good docstrings from day one\nTransition to MkDocs when README becomes insufficient\nGenerate API documentation from docstrings\n\nAutomate processes as repetition increases:\n\nAdd pre-commit hooks for local checks\nImplement CI for testing across environments\nAdd CD when deployment becomes routine\n\n\nRemember: Don’t overengineer! Choose the practices that add value to your specific project and team. It’s better to implement a few practices well than to poorly implement many."
  },
  {
    "objectID": "appendices/b-tools.html#environment-dependency-management",
    "href": "appendices/b-tools.html#environment-dependency-management",
    "title": "Appendix B — Appendix A: Python Development Tools Reference",
    "section": "B.1 Environment & Dependency Management",
    "text": "B.1 Environment & Dependency Management\n\nvenv: Python’s built-in tool for creating isolated virtual environments.\npip: The standard package installer for Python.\npip-tools: A set of tools for managing Python package dependencies with pinned versions via requirements.txt files.\nuv: A Rust-based, high-performance Python package manager and environment manager compatible with pip.\npipx: A tool for installing and running Python applications in isolated environments."
  },
  {
    "objectID": "appendices/b-tools.html#code-quality-formatting",
    "href": "appendices/b-tools.html#code-quality-formatting",
    "title": "Appendix B — Appendix A: Python Development Tools Reference",
    "section": "B.2 Code Quality & Formatting",
    "text": "B.2 Code Quality & Formatting\n\nRuff: A fast, Rust-based Python linter and formatter that consolidates multiple tools.\nBlack: An opinionated Python code formatter that enforces a consistent style.\nisort: A utility to sort Python imports alphabetically and automatically separate them into sections.\nFlake8: A code linting tool that checks Python code for style and logical errors.\nPylint: A comprehensive Python static code analyzer that looks for errors and enforces coding standards."
  },
  {
    "objectID": "appendices/b-tools.html#testing",
    "href": "appendices/b-tools.html#testing",
    "title": "Appendix B — Appendix A: Python Development Tools Reference",
    "section": "B.3 Testing",
    "text": "B.3 Testing\n\npytest: A powerful, flexible testing framework for Python that simplifies test writing and execution.\npytest-cov: A pytest plugin for measuring code coverage during test execution.\npytest-mock: A pytest plugin for creating and managing mock objects in tests."
  },
  {
    "objectID": "appendices/b-tools.html#type-checking",
    "href": "appendices/b-tools.html#type-checking",
    "title": "Appendix B — Appendix A: Python Development Tools Reference",
    "section": "B.4 Type Checking",
    "text": "B.4 Type Checking\n\nmypy: A static type checker for Python that helps catch type-related errors before runtime.\npydoc: Python’s built-in documentation generator and help system."
  },
  {
    "objectID": "appendices/b-tools.html#security-code-analysis",
    "href": "appendices/b-tools.html#security-code-analysis",
    "title": "Appendix B — Appendix A: Python Development Tools Reference",
    "section": "B.5 Security & Code Analysis",
    "text": "B.5 Security & Code Analysis\n\nBandit: A tool designed to find common security issues in Python code.\nVulture: A tool that detects unused code in Python programs."
  },
  {
    "objectID": "appendices/b-tools.html#documentation",
    "href": "appendices/b-tools.html#documentation",
    "title": "Appendix B — Appendix A: Python Development Tools Reference",
    "section": "B.6 Documentation",
    "text": "B.6 Documentation\n\nMkDocs: A fast and simple static site generator for building project documentation from Markdown files.\nmkdocs-material: A Material Design theme for MkDocs.\nmkdocstrings: A MkDocs plugin that automatically generates documentation from docstrings.\nSphinx: A comprehensive documentation tool that supports multiple output formats."
  },
  {
    "objectID": "appendices/b-tools.html#package-building-distribution",
    "href": "appendices/b-tools.html#package-building-distribution",
    "title": "Appendix B — Appendix A: Python Development Tools Reference",
    "section": "B.7 Package Building & Distribution",
    "text": "B.7 Package Building & Distribution\n\nbuild: A simple, correct PEP 517 package builder for Python projects.\ntwine: A utility for publishing Python packages to PyPI securely.\nsetuptools: The standard library for packaging Python projects.\nsetuptools-scm: A tool that manages your Python package versions using git metadata.\nwheel: A built-package format for Python that provides faster installation."
  },
  {
    "objectID": "appendices/b-tools.html#continuous-integration-deployment",
    "href": "appendices/b-tools.html#continuous-integration-deployment",
    "title": "Appendix B — Appendix A: Python Development Tools Reference",
    "section": "B.8 Continuous Integration & Deployment",
    "text": "B.8 Continuous Integration & Deployment\n\nGitHub Actions: GitHub’s built-in CI/CD platform for automating workflows.\npre-commit: A framework for managing and maintaining pre-commit hooks.\nCodecov: A tool for measuring and reporting code coverage in CI pipelines."
  },
  {
    "objectID": "appendices/b-tools.html#version-control",
    "href": "appendices/b-tools.html#version-control",
    "title": "Appendix B — Appendix A: Python Development Tools Reference",
    "section": "B.9 Version Control",
    "text": "B.9 Version Control\n\nGit: A distributed version control system for tracking changes in source code.\nGitHub/GitLab: Web-based platforms for hosting Git repositories with collaboration features."
  },
  {
    "objectID": "appendices/b-tools.html#project-setup-management",
    "href": "appendices/b-tools.html#project-setup-management",
    "title": "Appendix B — Appendix A: Python Development Tools Reference",
    "section": "B.10 Project Setup & Management",
    "text": "B.10 Project Setup & Management\n\nCookiecutter: A command-line utility that creates projects from templates, enabling consistent project setup with predefined structure and configurations. It uses a templating system to generate files and directories based on user inputs.\nGitHub Repository Templates: A GitHub feature that allows repositories to serve as templates for new projects. Users can generate new repositories with the same directory structure and files without needing to install additional tools. Unlike cookiecutter, GitHub templates don’t support parameterization but offer a zero-installation approach to project scaffolding."
  },
  {
    "objectID": "appendices/b-tools.html#advanced-tools",
    "href": "appendices/b-tools.html#advanced-tools",
    "title": "Appendix B — Appendix A: Python Development Tools Reference",
    "section": "B.11 Advanced Tools",
    "text": "B.11 Advanced Tools\n\nCython: A language that makes writing C extensions for Python as easy as writing Python.\nDocker: A platform for developing, shipping, and running applications in containers.\nKubernetes: An open-source system for automating deployment, scaling, and management of containerized applications.\nPants/Bazel: Build systems designed for monorepos and large codebases."
  },
  {
    "objectID": "appendices/c-glossary.html#a",
    "href": "appendices/c-glossary.html#a",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.1 A",
    "text": "C.1 A\n\nAPI (Application Programming Interface): A set of definitions and protocols for building and integrating application software.\nArtifact: Any file or package produced during the software development process, such as documentation or distribution packages."
  },
  {
    "objectID": "appendices/c-glossary.html#c",
    "href": "appendices/c-glossary.html#c",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.2 C",
    "text": "C.2 C\n\nCI/CD (Continuous Integration/Continuous Deployment): Practices where code changes are automatically tested (CI) and deployed to production (CD) when they pass quality checks.\nCLI (Command Line Interface): A text-based interface for interacting with software using commands.\nCode Coverage: A measure of how much of your code is executed during testing.\nCode Linting: The process of analyzing code for potential errors, style issues, and suspicious constructs."
  },
  {
    "objectID": "appendices/c-glossary.html#d",
    "href": "appendices/c-glossary.html#d",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.3 D",
    "text": "C.3 D\n\nDependency: An external package or module that your project requires to function properly.\nDocstring: A string literal specified in source code that is used to document a specific segment of code.\nDynamic Typing: A programming language feature where variable types are checked during runtime rather than compile time.\n\nCookiecutter: A project templating tool that helps developers create new projects with a predefined structure, configuration files, and boilerplate code. Cookiecutter uses Jinja2 templating to customize files based on user inputs during project generation."
  },
  {
    "objectID": "appendices/c-glossary.html#e",
    "href": "appendices/c-glossary.html#e",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.4 E",
    "text": "C.4 E\n\nEntry Point: A function or method that serves as an access point to an application, module, or library."
  },
  {
    "objectID": "appendices/c-glossary.html#f",
    "href": "appendices/c-glossary.html#f",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.5 F",
    "text": "C.5 F\n\nFixture: In testing, a piece of code that sets up a system for testing and provides test data."
  },
  {
    "objectID": "appendices/c-glossary.html#g",
    "href": "appendices/c-glossary.html#g",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.6 G",
    "text": "C.6 G\n\nGit: A distributed version control system for tracking changes in source code.\nGitHub Repository Template: A repository that can be used as a starting point for new projects on GitHub.\nGitHub/GitLab: Web-based platforms for hosting Git repositories with collaboration features."
  },
  {
    "objectID": "appendices/c-glossary.html#i",
    "href": "appendices/c-glossary.html#i",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.7 I",
    "text": "C.7 I\n\nIntegration Testing: Testing how different parts of the system work together."
  },
  {
    "objectID": "appendices/c-glossary.html#l",
    "href": "appendices/c-glossary.html#l",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.8 L",
    "text": "C.8 L\n\nLock File: A file that records the exact versions of dependencies needed by a project to ensure reproducible installations."
  },
  {
    "objectID": "appendices/c-glossary.html#m",
    "href": "appendices/c-glossary.html#m",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.9 M",
    "text": "C.9 M\n\nMocking: Simulating the behavior of real objects in controlled ways during testing.\nModule: A file containing Python code that can be imported and used by other Python files.\nMonorepo: A software development strategy where many projects are stored in the same repository."
  },
  {
    "objectID": "appendices/c-glossary.html#n",
    "href": "appendices/c-glossary.html#n",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.10 N",
    "text": "C.10 N\n\nNamespace Package: A package split across multiple directories or distribution packages."
  },
  {
    "objectID": "appendices/c-glossary.html#p",
    "href": "appendices/c-glossary.html#p",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.11 P",
    "text": "C.11 P\n\nPackage: A directory of Python modules containing an additional __init__.py file.\nPEP (Python Enhancement Proposal): A design document providing information to the Python community, often proposing new features.\nPEP 8: The style guide for Python code.\nPyPI (Python Package Index): The official repository for third-party Python software."
  },
  {
    "objectID": "appendices/c-glossary.html#r",
    "href": "appendices/c-glossary.html#r",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.12 R",
    "text": "C.12 R\n\nRefactoring: Restructuring existing code without changing its external behavior.\nRepository: A storage location for software packages and version control.\nRequirements File: A file listing the dependencies required for a Python project.\nReproducible Build: A build that can be recreated exactly regardless of when or where it’s built."
  },
  {
    "objectID": "appendices/c-glossary.html#s",
    "href": "appendices/c-glossary.html#s",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.13 S",
    "text": "C.13 S\n\nSemantic Versioning: A versioning scheme in the format MAJOR.MINOR.PATCH, where each number increment indicates the type of change.\nStatic Analysis: Analyzing code without executing it to find potential issues.\nStatic Typing: Specifying variable types at compile time instead of runtime.\nStub Files: Files that contain type annotations for modules that don’t have native typing support."
  },
  {
    "objectID": "appendices/c-glossary.html#t",
    "href": "appendices/c-glossary.html#t",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.14 T",
    "text": "C.14 T\n\nTest-Driven Development (TDD): A development process where tests are written before the code.\nType Annotation: Syntax for indicating the expected type of variables, function parameters, and return values.\nType Hinting: Adding type annotations to Python code to help with static analysis and IDE assistance."
  },
  {
    "objectID": "appendices/c-glossary.html#u",
    "href": "appendices/c-glossary.html#u",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.15 U",
    "text": "C.15 U\n\nUnit Testing: Testing individual components in isolation from the rest of the system."
  },
  {
    "objectID": "appendices/c-glossary.html#v",
    "href": "appendices/c-glossary.html#v",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.16 V",
    "text": "C.16 V\n\nVirtual Environment: An isolated Python environment that allows packages to be installed for use by a particular project, without affecting other projects."
  },
  {
    "objectID": "appendices/c-glossary.html#w",
    "href": "appendices/c-glossary.html#w",
    "title": "Appendix C — Appendix B: Glossary of Python Development Terms",
    "section": "C.17 W",
    "text": "C.17 W\n\nWheel: A built-package format for Python that can be installed more quickly than source distributions."
  },
  {
    "objectID": "appendices/d-bash-scaffold-script.html",
    "href": "appendices/d-bash-scaffold-script.html",
    "title": "Appendix D — Appendix D: Python Development Pipeline Scaffold Python Script",
    "section": "",
    "text": "#!/bin/bash\n# scaffold_python_project.sh - A simple script to create a Python project with best practices\n# Usage: ./scaffold_python_project.sh my_project\n\nif [ -z \"$1\" ]; then\n  echo \"Please provide a project name.\"\n  echo \"Usage: ./scaffold_python_project.sh my_project\"\n  exit 1\nfi\n\nPROJECT_NAME=$1\n# Convert hyphens to underscores for Python package naming conventions\nPACKAGE_NAME=$(echo $PROJECT_NAME | tr '-' '_')\n\necho \"Creating project: $PROJECT_NAME\"\necho \"Package name will be: $PACKAGE_NAME\"\n\n# Create project directory\nmkdir -p $PROJECT_NAME\ncd $PROJECT_NAME\n\n# Create basic structure following the recommended src layout\n# The src layout enforces proper package installation and creates clear boundaries\nmkdir -p src/$PACKAGE_NAME tests docs\n\n# Create package files\n# __init__.py makes the directory a Python package\ntouch src/$PACKAGE_NAME/__init__.py\ntouch src/$PACKAGE_NAME/main.py\n\n# Create test files - keeping tests separate but adjacent to the implementation\n# This follows the principle of separating implementation from tests\ntouch tests/__init__.py\ntouch tests/test_main.py\n\n# Create documentation placeholder - establishing documentation from the start\n# Even minimal docs are better than no docs\necho \"# $PROJECT_NAME Documentation\" &gt; docs/index.md\n\n# Create README.md with basic information\n# README is the first document anyone sees and should provide clear instructions\necho \"# $PROJECT_NAME\n\nA Python project created with best practices.\n\n## Installation\n\n\\`\\`\\`bash\npip install $PROJECT_NAME\n\\`\\`\\`\n\n## Usage\n\n\\`\\`\\`python\nfrom $PACKAGE_NAME import main\n\\`\\`\\`\n\n## Development\n\n\\`\\`\\`bash\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\\\Scripts\\\\activate\n\n# Install in development mode\npip install -e .\n\n# Run tests\npytest\n\\`\\`\\`\n\" &gt; README.md\n\n# Create .gitignore file to exclude unnecessary files from version control\n# This prevents committing files that should not be in the repository\necho \"# Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# Virtual environments\n# Never commit virtual environments to version control\n.venv/\nvenv/\nENV/\n\n# Testing\n.pytest_cache/\n.coverage\nhtmlcov/\n\n# Documentation\ndocs/_build/\n\n# IDE\n.idea/\n.vscode/\n*.swp\n*.swo\n\" &gt; .gitignore\n\n# Create pyproject.toml for modern Python packaging\n# This follows PEP 517/518 standards and centralizes project configuration\necho \"[build-system]\nrequires = [\\\"setuptools&gt;=61.0\\\", \\\"wheel\\\"]\nbuild-backend = \\\"setuptools.build_meta\\\"\n\n[project]\nname = \\\"$PROJECT_NAME\\\"\nversion = \\\"0.1.0\\\"\ndescription = \\\"A Python project created with best practices\\\"\nreadme = \\\"README.md\\\"\nrequires-python = \\\"&gt;=3.8\\\"\nauthors = [\n    {name = \\\"Your Name\\\", email = \\\"your.email@example.com\\\"}\n]\n\n[project.urls]\n\\\"Homepage\\\" = \\\"https://github.com/yourusername/$PROJECT_NAME\\\"\n\n# Specify the src layout for better package isolation\n[tool.setuptools]\npackage-dir = {\\\"\\\" = \\\"src\\\"}\npackages = [\\\"$PACKAGE_NAME\\\"]\n\n# Configure pytest to look in the tests directory\n[tool.pytest.ini_options]\ntestpaths = [\\\"tests\\\"]\n\" &gt; pyproject.toml\n\n# Create requirements.in for direct dependencies\n# This approach is cleaner than freezing everything with pip freeze\necho \"# Project dependencies\n# Add your dependencies here, e.g.:\n# requests&gt;=2.25.0\n\" &gt; requirements.in\n\n# Create example main.py with docstrings and type hints\n# Starting with good documentation and typing practices from the beginning\necho \"\\\"\\\"\\\"Main module for $PROJECT_NAME.\\\"\\\"\\\"\n\ndef example_function(text: str) -&gt; str:\n    \\\"\\\"\\\"Return a greeting message.\n\n    Args:\n        text: The text to include in the greeting.\n\n    Returns:\n        A greeting message.\n    \\\"\\\"\\\"\n    return f\\\"Hello, {text}!\\\"\n\" &gt; src/$PACKAGE_NAME/main.py\n\n# Create example test file\n# Tests verify that code works as expected and prevent regressions\necho \"\\\"\\\"\\\"Tests for the main module.\\\"\\\"\\\"\n\nfrom $PACKAGE_NAME.main import example_function\n\ndef test_example_function():\n    \\\"\\\"\\\"Test the example function returns the expected greeting.\\\"\\\"\\\"\n    result = example_function(\\\"World\\\")\n    assert result == \\\"Hello, World!\\\"\n\" &gt; tests/test_main.py\n\n# Initialize git repository\n# Version control should be established from the very beginning\ngit init\ngit add .\ngit commit -m \"Initial project setup\"\n\necho \"\"\necho \"Project $PROJECT_NAME created successfully!\"\necho \"\"\necho \"Next steps:\"\necho \"1. cd $PROJECT_NAME\"\necho \"2. python -m venv .venv\"\necho \"3. source .venv/bin/activate  # On Windows: .venv\\\\Scripts\\\\activate\"\necho \"4. pip install -e .\"\necho \"5. pytest\"\necho \"\"\necho \"Happy coding!\""
  },
  {
    "objectID": "appendices/e-cookiecutter.html#what-is-cookiecutter",
    "href": "appendices/e-cookiecutter.html#what-is-cookiecutter",
    "title": "Appendix E — Appendix E: Python Development Pipeline Cookiecutter Template",
    "section": "E.1 What is Cookiecutter?",
    "text": "E.1 What is Cookiecutter?\nCookiecutter is a command-line utility that creates projects from templates. It takes a template directory containing a cookiecutter.json file with template variables and replaces them with user-provided values, generating a project directory structure with all necessary files."
  },
  {
    "objectID": "appendices/e-cookiecutter.html#getting-started-with-the-template",
    "href": "appendices/e-cookiecutter.html#getting-started-with-the-template",
    "title": "Appendix E — Appendix E: Python Development Pipeline Cookiecutter Template",
    "section": "E.2 Getting Started with the Template",
    "text": "E.2 Getting Started with the Template\n\nE.2.1 Prerequisites\n\nPython 3.7 or later\npip package manager\n\n\n\nE.2.2 Installation\nFirst, install cookiecutter:\npip install cookiecutter\n\n\nE.2.3 Creating a New Project\nTo create a new project using our Python Development Pipeline template:\ncookiecutter gh:username/python-dev-pipeline-cookiecutter\nYou’ll be prompted to provide information about your project, such as:\n\nProject name\nAuthor information\nPython version requirements\nLicense type\nDevelopment level (basic or advanced)\nDocumentation preferences\nCI/CD preferences\nPackage manager choice (pip-tools or uv)\n\nAfter answering these questions, cookiecutter will generate a complete project structure with all the configuration files and setup based on your choices."
  },
  {
    "objectID": "appendices/e-cookiecutter.html#template-features",
    "href": "appendices/e-cookiecutter.html#template-features",
    "title": "Appendix E — Appendix E: Python Development Pipeline Cookiecutter Template",
    "section": "E.3 Template Features",
    "text": "E.3 Template Features\nThe template implements all the best practices discussed throughout this book:\n\nE.3.1 Project Structure\n\nUses the recommended src layout for better package isolation\nProperly organized test directory\nDocumentation setup with MkDocs (if selected)\nClear separation of concerns across files and directories\n\n\n\nE.3.2 Development Environment\n\nConfigured virtual environment instructions\nDependency management using either pip-tools or uv\nrequirements.in and requirements-dev.in files for clean dependency specification\n\n\n\nE.3.3 Code Quality Tools\n\nRuff for formatting and linting\nmypy for type checking\nBandit for security analysis (with advanced setup)\nPre-configured with sensible defaults in pyproject.toml\n\n\n\nE.3.4 Testing\n\npytest setup with example tests\nCoverage configuration\nTest helper fixtures\n\n\n\nE.3.5 Documentation\n\nMkDocs with Material theme (if selected)\nAPI documentation generation with mkdocstrings\nTemplate pages for quickstart, examples, and API reference\n\n\n\nE.3.6 CI/CD\n\nGitHub Actions workflows for testing, linting, and type checking\nPublish workflow for PyPI deployment\nMatrix testing across Python versions"
  },
  {
    "objectID": "appendices/e-cookiecutter.html#customization-options",
    "href": "appendices/e-cookiecutter.html#customization-options",
    "title": "Appendix E — Appendix E: Python Development Pipeline Cookiecutter Template",
    "section": "E.4 Customization Options",
    "text": "E.4 Customization Options\nThe template offers several customization options during generation:\n\nE.4.1 Basic vs. Advanced Setup\n\nBasic: Lighter configuration focused on essential tools\nAdvanced: Full suite of tools including security scanning, stricter type checking, and comprehensive CI/CD\n\n\n\nE.4.2 Documentation Options\n\nChoose whether to include MkDocs documentation setup\nIf included, get a complete documentation structure ready for content\n\n\n\nE.4.3 CI/CD Options\n\nInclude GitHub Actions workflows for automated testing and deployment\nConfigure publishing workflows for PyPI integration"
  },
  {
    "objectID": "appendices/e-cookiecutter.html#template-structure",
    "href": "appendices/e-cookiecutter.html#template-structure",
    "title": "Appendix E — Appendix E: Python Development Pipeline Cookiecutter Template",
    "section": "E.5 Template Structure",
    "text": "E.5 Template Structure\nThe generated project follows this structure:\nyour_project/\n├── .github/                        # GitHub specific configuration\n│   └── workflows/                  # GitHub Actions workflows\n│       ├── ci.yml                  # Continuous Integration workflow\n│       └── publish.yml             # Package publishing workflow\n├── src/                            # Main source code directory\n│   └── your_package/               # Actual Python package\n│       ├── __init__.py             # Makes the directory a package\n│       └── main.py                 # Example module\n├── tests/                          # Test suite\n│   ├── __init__.py                 # Makes tests importable\n│   └── test_main.py                # Tests for main.py\n├── docs/                           # Documentation\n│   ├── index.md                    # Main documentation page\n│   └── examples.md                 # Example usage\n├── .gitignore                      # Files to exclude from Git\n├── LICENSE                         # License file\n├── README.md                       # Project overview\n├── requirements.in                 # Direct dependencies (human-maintained)\n├── requirements-dev.in             # Development dependencies\n└── pyproject.toml                  # Project & tool configuration"
  },
  {
    "objectID": "appendices/e-cookiecutter.html#post-generation-steps",
    "href": "appendices/e-cookiecutter.html#post-generation-steps",
    "title": "Appendix E — Appendix E: Python Development Pipeline Cookiecutter Template",
    "section": "E.6 Post-Generation Steps",
    "text": "E.6 Post-Generation Steps\nAfter creating your project, the template provides instructions for:\n\nCreating and activating a virtual environment\nInstalling dependencies\nSetting up version control\nRunning initial tests\n\nThe generated README.md includes detailed development setup instructions specific to your configuration choices."
  },
  {
    "objectID": "appendices/e-cookiecutter.html#extending-the-template",
    "href": "appendices/e-cookiecutter.html#extending-the-template",
    "title": "Appendix E — Appendix E: Python Development Pipeline Cookiecutter Template",
    "section": "E.7 Extending the Template",
    "text": "E.7 Extending the Template\nYou can extend or customize the template for your specific needs:\n\nE.7.1 Adding Custom Components\nFork the template repository and add additional files or configurations specific to your organization or preferences.\n\n\nE.7.2 Modifying Tool Configurations\nThe pyproject.toml file contains all tool configurations and can be adjusted to match your coding standards and preferences.\n\n\nE.7.3 Creating Specialized Variants\nCreate specialized variants of the template for different types of projects (e.g., web applications, data science, CLI tools) while maintaining the core best practices."
  },
  {
    "objectID": "appendices/e-cookiecutter.html#best-practices-for-using-the-template",
    "href": "appendices/e-cookiecutter.html#best-practices-for-using-the-template",
    "title": "Appendix E — Appendix E: Python Development Pipeline Cookiecutter Template",
    "section": "E.8 Best Practices for Using the Template",
    "text": "E.8 Best Practices for Using the Template\n\nUse for new projects: The template is designed for new projects rather than retrofitting existing ones.\nCommit immediately after generation: Make an initial commit right after generating the project to establish a clean baseline.\nReview and adjust configurations: While the defaults are sensible, review and adjust configurations to match your specific project needs.\nKeep dependencies updated: Regularly update the requirements.in files as your project evolves.\nFollow the workflow: The template sets up the infrastructure, but you still need to follow the development workflow described in this book."
  },
  {
    "objectID": "appendices/e-cookiecutter.html#conclusion",
    "href": "appendices/e-cookiecutter.html#conclusion",
    "title": "Appendix E — Appendix E: Python Development Pipeline Cookiecutter Template",
    "section": "E.9 Conclusion",
    "text": "E.9 Conclusion\nThe Python Development Pipeline cookiecutter template encapsulates the practices and principles discussed throughout this book, allowing you to rapidly bootstrap projects with best practices already in place. By using this template, you ensure consistency across projects and can focus more on solving problems rather than setting up infrastructure.\nWhether you’re starting a small personal project or a larger team effort, this template provides a solid foundation that can scale with your needs while maintaining professional development standards."
  },
  {
    "objectID": "appendices/f-ai-tools.html#overview-of-current-ai-tools-and-their-strengths",
    "href": "appendices/f-ai-tools.html#overview-of-current-ai-tools-and-their-strengths",
    "title": "Appendix F — Appendix F: AI Tools for Python Development",
    "section": "F.1 Overview of Current AI Tools and Their Strengths",
    "text": "F.1 Overview of Current AI Tools and Their Strengths\n\nF.1.1 Code Assistants and Completion Tools\n\nGitHub Copilot:\n\nStrengths: Real-time code suggestions directly in your IDE; trained on public GitHub repositories; understands context from open files\nBest for: Rapid code generation, boilerplate reduction, exploring implementation alternatives\nIntegration: Available for VS Code, Visual Studio, JetBrains IDEs, and Neovim\n\nJetBrains AI Assistant:\n\nStrengths: Deeply integrated with JetBrains IDEs; code explanation and generation; documentation creation\nBest for: PyCharm users; explaining complex code; generating docstrings\nIntegration: Built into PyCharm and other JetBrains products\n\nTabnine:\n\nStrengths: Code completion with local models option; privacy-focused; adapts to your coding style\nBest for: Teams with strict data privacy requirements; personalized code suggestions\nIntegration: Works with most popular IDEs including VS Code and PyCharm\n\n\n\n\nF.1.2 Conversational AI Assistants\n\nClaude (Anthropic):\n\nStrengths: Excellent reasoning capabilities; strong Python knowledge; handles lengthy context\nBest for: Complex problem-solving; explaining algorithms; reviewing code; documentation creation\nAccess: Web interface, API, Claude Code (terminal)\n\nChatGPT/GPT-4 (OpenAI):\n\nStrengths: Wide knowledge base; good at generating code and explaining concepts\nBest for: Troubleshooting; learning concepts; brainstorming ideas; code generation\nAccess: Web interface, API, plugins for various platforms\n\nGemini (Google):\n\nStrengths: Strong code analysis and generation; multimodal capabilities useful for analyzing diagrams\nBest for: Code support; learning resources; teaching concepts\nAccess: Web interface, API, Duet AI integrations\n\n\n\n\nF.1.3 AI-Enhanced Code Review Tools\n\nDeepSource:\n\nStrengths: Continuous analysis; focuses on security issues, anti-patterns, and performance\nBest for: Automated code reviews; maintaining code quality standards\nIntegration: GitHub, GitLab, Bitbucket\n\nCodiga:\n\nStrengths: Real-time code analysis; custom rule creation; automated PR comments\nBest for: Enforcing team-specific best practices; providing quick feedback\nIntegration: GitHub, GitLab, Bitbucket, and various IDEs\n\nSourcery:\n\nStrengths: Python-specific refactoring suggestions; explains why changes are recommended\nBest for: Learning better Python patterns; gradual code quality improvement\nIntegration: VS Code, JetBrains IDEs, GitHub\n\n\n\n\nF.1.4 AI Documentation Tools\n\nMintlify Writer:\n\nStrengths: Auto-generates documentation from code; supports various docstring formats\nBest for: Quickly documenting existing codebases; maintaining consistent documentation\nIntegration: VS Code, JetBrains IDEs\n\nDocstring Generator AI:\n\nStrengths: Creates detailed docstrings following specified formats (Google, NumPy, etc.)\nBest for: Consistently formatting documentation across a project\nIntegration: VS Code extension"
  },
  {
    "objectID": "appendices/f-ai-tools.html#guidelines-for-effective-prompting",
    "href": "appendices/f-ai-tools.html#guidelines-for-effective-prompting",
    "title": "Appendix F — Appendix F: AI Tools for Python Development",
    "section": "F.2 Guidelines for Effective Prompting",
    "text": "F.2 Guidelines for Effective Prompting\nThe quality of AI output depends significantly on how you formulate your requests. Here are strategies to get the most from AI tools when working with Python:\n\nF.2.1 General Prompting Principles\n\nBe specific and detailed: Include relevant context about your project, such as Python version, frameworks used, and existing patterns to follow.\n# Less effective\n\"Write a function to process user data.\"\n\n# More effective\n\"Write a Python 3.10 function to process user data that:\n- Takes a dictionary of user attributes\n- Validates email and age fields\n- Returns a normalized user object\n- Follows our project's error handling pattern of raising ValueError with descriptive messages\n- Uses type hints\"\nProvide examples: When you need code that follows certain patterns or styles, provide examples.\n\"Here's how we write API handler functions in our project:\n\nasync def get_user(user_id: int) -&gt; Dict[str, Any]:\n    try:\n        response = await http_client.get(f\"/users/{user_id}\")\n        return response.json()\n    except HTTPError as e:\n        log.error(f\"Failed to fetch user {user_id}: {e}\")\n        raise UserFetchError(f\"Could not retrieve user: {e}\")\n\nPlease write a similar function for fetching user orders.\"\nUse iterative refinement: Start with a basic request, then refine the results.\n# Initial prompt\n\"Write a function to parse CSV files with pandas.\"\n\n# Follow-up refinements\n\"Now add error handling for missing files.\"\n\"Update it to support both comma and semicolon delimiters.\"\n\"Add type hints to the function.\"\nSpecify output format: Clarify how you want information presented.\n\"Explain the difference between @classmethod and @staticmethod in Python.\nFormat your response with:\n1. A brief definition of each\n2. Code examples showing typical use cases\n3. A table comparing their key attributes\"\n\n\n\nF.2.2 Python-Specific Prompting Strategies\n\nRequest specific Python versions or features: Clarify which Python version you’re targeting.\n\"Write this function using Python 3.9+ features like the new dictionary merge operator.\"\nSpecify testing frameworks: When requesting tests, mention your preferred framework.\n\"Generate pytest test cases for this function, using fixtures and parametrize for the test scenarios.\"\nAsk for alternative approaches: Python often offers multiple solutions to problems.\n\"Show three different ways to implement this list filtering function, explaining the tradeoffs between readability, performance, and memory usage.\"\nRequest educational explanations: For learning purposes, ask the AI to explain its reasoning.\n\"Write a function to efficiently find duplicate elements in a list, then explain why the algorithm you chose is efficient and what its time complexity is.\"\n\n\n\nF.2.3 Using AI for Code Review\nWhen using AI to review your Python code, structured prompts yield better results:\n\"Review this Python code for:\n1. Potential bugs or edge cases\n2. Performance issues\n3. Pythonic improvements\n4. PEP 8 compliance\n5. Possible security concerns\n\n```python\ndef process_user_input(data):\n    # [your code here]\nFor each issue found, please: - Describe the problem - Explain why it’s problematic - Suggest a specific improvement with code”\n\n### Troubleshooting with AI\n\nWhen debugging problems, provide context systematically:\n\n“I’m getting this error when running my Python script:\n[Error message]\nHere’s the relevant code:\n# [your code here]\nI’ve already tried: 1. [attempted solution 1] 2. [attempted solution 2]\nI’m using Python 3.9 with packages: pandas 1.5.3, numpy 1.23.0\nWhat might be causing this error and how can I fix it?” ```"
  },
  {
    "objectID": "appendices/f-ai-tools.html#ethical-considerations-and-limitations",
    "href": "appendices/f-ai-tools.html#ethical-considerations-and-limitations",
    "title": "Appendix F — Appendix F: AI Tools for Python Development",
    "section": "F.3 Ethical Considerations and Limitations",
    "text": "F.3 Ethical Considerations and Limitations\nAs you integrate AI tools into your Python development workflow, consider these important ethical considerations and limitations:\n\nF.3.1 Ethical Considerations\n\nIntellectual Property and Licensing\n\nCode generated by AI may be influenced by training data with various licenses\nFor commercial projects, consult your legal team about AI code usage policies\nConsider adding comments attributing AI-generated sections when substantial\n\nSecurity Risks\n\nNever blindly implement AI-suggested security-critical code without review\nAI may recommend outdated or vulnerable patterns it learned from older code\nVerify cryptographic implementations, authentication mechanisms, and input validation independently\n\nOverreliance and Skill Development\n\nBalance AI usage with developing personal understanding\nFor educational settings, consider policies on appropriate AI assistance\nUse AI to enhance learning rather than bypass it\n\nBias and Fairness\n\nAI may perpetuate biases present in training data\nReview generated code for potential unfair treatment or assumptions\nBe especially careful with user-facing features and data processing pipelines\n\nEnvironmental Impact\n\nLarge AI models have significant computational and energy costs\nConsider using more efficient, specialized code tools for routine tasks\nBatch similar requests when possible instead of making many small queries\n\n\n\n\nF.3.2 Technical Limitations\n\nKnowledge Cutoffs\n\nAI assistants have knowledge cutoffs and may not be aware of recent Python developments\nVerify suggestions for newer Python versions or recently updated libraries\nExample: An AI might not know about features introduced in Python 3.11 or 3.12 if its training cutoff predates them\n\nContext Length Restrictions\n\nMost AI tools have limits on how much code they can process at once\nFor large files or complex projects, focus queries on specific components\nProvide essential context rather than entire codebases\n\nHallucinations and Inaccuracies\n\nAI can confidently suggest incorrect implementations or non-existent functions\nAlways verify generated code works as expected\nBe especially wary of package import suggestions, API usage patterns, and framework-specific code\n\nUnderstanding Project-Specific Context\n\nAI lacks full understanding of your project architecture and requirements\nGenerated code may not align with your established patterns or constraints\nAlways review for compatibility with your broader codebase\n\nTime-Sensitive Information\n\nBest practices, dependencies, and security recommendations change over time\nVerify suggestions against current Python community standards\nDouble-check deprecation warnings and avoid outdated patterns\n\n\n\n\nF.3.3 Practical Mitigation Strategies\n\nCode Review Process\n\nEstablish clear guidelines for reviewing AI-generated code\nUse the same quality standards for AI-generated and human-written code\nConsider automated testing requirements for AI contributions\n\nAttribution and Documentation\n\nDocument where and how AI tools were used in your development process\nConsider noting substantial AI contributions in code comments\nExample: # Initial implementation generated by GitHub Copilot, modified to handle edge cases\n\nVerification Practices\n\nTest AI-generated code thoroughly, especially edge cases\nVerify performance characteristics claimed by AI suggestions\nCross-check security recommendations with trusted sources\n\nBalanced Use Policy\n\nDevelop team guidelines for appropriate AI tool usage\nEncourage use for boilerplate, documentation, and creative starting points\nEmphasize human oversight for architecture, security, and critical algorithms\n\nContinuous Learning\n\nUse AI explanations as learning opportunities\nAsk AI to explain its suggestions and verify understanding\nBuild knowledge to reduce dependency on AI for core concepts"
  },
  {
    "objectID": "appendices/f-ai-tools.html#the-future-of-ai-in-python-development",
    "href": "appendices/f-ai-tools.html#the-future-of-ai-in-python-development",
    "title": "Appendix F — Appendix F: AI Tools for Python Development",
    "section": "F.4 The Future of AI in Python Development",
    "text": "F.4 The Future of AI in Python Development\nAI tools for Python development are evolving rapidly. Current trends suggest these future directions:\n\nMore specialized Python-specific models: Trained specifically on Python codebases with deeper framework understanding\nEnhanced IDE integration: More seamless AI assistance throughout the development workflow\nImproved testing capabilities: AI generating more comprehensive test suites with higher coverage\nCustom models for organizations: Trained on internal codebases to better match company standards\nAgent-based development: AI systems that can execute multi-step development tasks with minimal guidance\n\nAs these tools evolve, maintaining a balanced approach that leverages AI strengths while preserving human oversight will remain essential for quality Python development."
  }
]